"""
SQL Query Evaluator Dashboard
A Streamlit application to validate SQL queries generated by chatbots against provided context.
"""

import streamlit as st
import pandas as pd
import json
import yaml
from io import StringIO, BytesIO
import time
from datetime import datetime
from openai import OpenAI, AzureOpenAI
from collections import Counter
from pdf_utils import markdown_to_pdf_bytes

# Page configuration
st.set_page_config(
    page_title="SQL Query Evaluator",
    page_icon="üîç",
    layout="wide",
    initial_sidebar_state="expanded"
)

# Custom CSS for better styling
st.markdown("""
<style>
    .main-header {
        font-size: 2.5rem !important;
        font-weight: bold !important;
        color: #1f77b4 !important;
        text-align: center !important;
        margin-bottom: 1rem !important;
        line-height: 1.2 !important;
    }
    p.main-header {
        font-size: 2.5rem !important;
    }
    .sub-header {
        font-size: 1.4rem !important;
        color: #666 !important;
        text-align: center !important;
        margin-bottom: 2rem !important;
        line-height: 1.3 !important;
    }
    p.sub-header {
        font-size: 1.4rem !important;
    }
    .metric-card {
        background-color: #f0f2f6;
        border-radius: 10px;
        padding: 1rem;
        text-align: center;
    }
    .pass-badge {
        background-color: #28a745;
        color: white;
        padding: 0.25rem 0.5rem;
        border-radius: 4px;
        font-weight: bold;
    }
    .fail-badge {
        background-color: #dc3545;
        color: white;
        padding: 0.25rem 0.5rem;
        border-radius: 4px;
        font-weight: bold;
    }
    .stTabs [data-baseweb="tab-list"] {
        gap: 24px;
    }
    .stTabs [data-baseweb="tab"] {
        height: 50px;
        padding-left: 20px;
        padding-right: 20px;
    }
    .context-section {
        border: 1px solid #ddd;
        border-radius: 8px;
        padding: 1rem;
        margin-bottom: 1rem;
        background-color: #fafafa;
    }
    .results-summary {
        font-size: 1.5rem;
        font-weight: bold;
        padding: 1rem;
        border-radius: 10px;
        text-align: center;
        margin: 1rem 0;
    }
    /* Compact sidebar styling */
    section[data-testid="stSidebar"] > div {
        padding-top: 0.5rem;
    }
    section[data-testid="stSidebar"] .stMarkdown {
        margin-bottom: 0.25rem;
    }
    section[data-testid="stSidebar"] h3 {
        margin-top: 0.25rem;
        margin-bottom: 0.25rem;
    }
    section[data-testid="stSidebar"] .stButton {
        margin-top: 0.25rem;
        margin-bottom: 0.25rem;
    }
    section[data-testid="stSidebar"] .stCheckbox {
        margin-bottom: 0.25rem;
    }
    section[data-testid="stSidebar"] .stRadio {
        margin-bottom: 0.25rem;
    }
    section[data-testid="stSidebar"] .stTextInput {
        margin-bottom: 0.25rem;
    }
    section[data-testid="stSidebar"] .stSelectbox {
        margin-bottom: 0.25rem;
    }
    section[data-testid="stSidebar"] .stFileUploader {
        margin-bottom: 0.5rem;
    }
    section[data-testid="stSidebar"] div[data-baseweb="input"] {
        margin-bottom: 0;
    }
    section[data-testid="stSidebar"] .element-container {
        margin-bottom: 0.25rem;
    }
    section[data-testid="stSidebar"] hr {
        margin-top: 0.5rem;
        margin-bottom: 0.5rem;
    }
</style>
""", unsafe_allow_html=True)

# Initialize session state
if 'evaluation_results' not in st.session_state:
    st.session_state.evaluation_results = None
if 'questions_data' not in st.session_state:
    st.session_state.questions_data = []
if 'context_data' not in st.session_state:
    st.session_state.context_data = {
        'business_rules': None,
        'kpi_formulas': None,
        'data_dictionary': None,
        'table_samples': {}
    }
if 'openai_api_key' not in st.session_state:
    st.session_state.openai_api_key = ""
if 'azure_api_key' not in st.session_state:
    st.session_state.azure_api_key = ""
if 'api_provider' not in st.session_state:
    st.session_state.api_provider = "OpenAI"
if 'azure_base_url' not in st.session_state:
    st.session_state.azure_base_url = ""
if 'azure_api_version' not in st.session_state:
    st.session_state.azure_api_version = "2024-02-01"
if 'evaluation_history' not in st.session_state:
    st.session_state.evaluation_history = []
if 'use_quick_start' not in st.session_state:
    st.session_state.use_quick_start = False
if 'quick_start_loaded' not in st.session_state:
    st.session_state.quick_start_loaded = False


# =============================================================================
# QUICK START HELPER FUNCTIONS
# =============================================================================

def load_quick_start_configuration():
    """Load default configuration for quick start. Returns list of loaded files."""
    import os

    loaded_files = {
        'business_rules': [],
        'kpi_formulas': [],
        'data_dictionary': [],
        'sample_questions': []
    }

    # Set API provider to Azure OpenAI
    st.session_state.api_provider = "Azure OpenAI"

    # Load Azure OpenAI credentials from secrets
    if hasattr(st, 'secrets'):
        st.session_state.azure_api_key = st.secrets.get("AZURE_OPENAI_API_KEY", "")
        st.session_state.azure_base_url = st.secrets.get("AZURE_OPENAI_API_BASE", "")

    # Set default model and API version
    st.session_state.azure_api_version = "2024-02-01"

    # Load default context files
    base_path = os.path.join(os.getcwd(), "Chatbot Context")

    # Load .md files as business rules
    business_rules = []
    for md_file in ["vanna.md", "location_utils.md", "prompts.md"]:
        file_path = os.path.join(base_path, md_file)
        if os.path.exists(file_path):
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
                business_rules.append({'file': md_file, 'content': content})
                loaded_files['business_rules'].append(md_file)

    if business_rules:
        st.session_state.context_data['business_rules'] = business_rules

    # Load doc.yaml for KPI formulas
    doc_yaml_path = os.path.join(base_path, "doc (11,115 tokens).yaml")
    if os.path.exists(doc_yaml_path):
        with open(doc_yaml_path, 'r', encoding='utf-8') as f:
            content = f.read()
            parsed = parse_yaml_content(content)
            if parsed:
                st.session_state.context_data['kpi_formulas'] = [{'file': 'doc (11,115 tokens).yaml', 'content': parsed}]
                loaded_files['kpi_formulas'].append('doc (11,115 tokens).yaml')

    # Load ddl.yaml for data dictionary
    ddl_yaml_path = os.path.join(base_path, "ddl (6,412 tokens).yaml")
    if os.path.exists(ddl_yaml_path):
        with open(ddl_yaml_path, 'r', encoding='utf-8') as f:
            content = f.read()
            parsed = parse_yaml_content(content)
            if parsed:
                st.session_state.context_data['data_dictionary'] = [{'file': 'ddl (6,412 tokens).yaml', 'content': parsed}]
                loaded_files['data_dictionary'].append('ddl (6,412 tokens).yaml')

    # Load sql-examples_10_test.csv for sample questions
    csv_path = os.path.join(base_path, "sql-examples_10_test.csv")
    if os.path.exists(csv_path):
        df = pd.read_csv(csv_path)
        if 'question' in df.columns and 'sql_query' in df.columns:
            st.session_state.questions_data = df.to_dict('records')
            loaded_files['sample_questions'].append(f'sql-examples_10_test.csv ({len(df)} questions)')

    # Note: Table samples are intentionally NOT loaded in quick start
    # Users can manually upload table samples if needed

    st.session_state.quick_start_loaded = True

    return loaded_files


# =============================================================================
# EVALUATION PROMPT TEMPLATE
# =============================================================================

EVALUATION_SYSTEM_PROMPT = """You are an expert SQL Query Evaluator. Your task is to evaluate whether a generated SQL query correctly answers the given user question based on the provided context (business rules, KPI formulas, data dictionary, and table samples).

## Your Evaluation Criteria:

1. **Correctness**: Does the SQL query correctly answer the user's question?
2. **Table Selection**: Are the correct tables being used based on the data dictionary?
3. **Formula Accuracy**: Are KPI calculations using the correct formulas as defined in the KPI formulas?
4. **Business Rule Compliance**: Does the query follow all business rules (e.g., filtering rules, exclusions)?
5. **Column Usage**: Are the correct columns being selected and filtered?
6. **Aggregation Logic**: Is the aggregation level appropriate for the question?
7. **Join Logic**: Are table joins correct and necessary?
8. **Filter Conditions**: Are WHERE clauses appropriate and complete?

## Common Issue Types (Reference Guide)

When evaluating queries, look for these common issue patterns (but not limited to these):

**Time & Date Logic:**
- timeframe_mismatch: Query uses wrong timeframe that doesn't match user intent (e.g., wrong month, quarter, or year)
- incorrect_to_date_logic: Wrong MTD/QTD/YTD calculation or period boundaries
- incomplete_period_coverage: Partial months/quarters/weeks when full period expected
- ambiguous_date_column: Wrong date column used for filtering or aggregation
- incorrect_date_boundaries: Wrong date range boundaries (e.g., exclusive vs inclusive, off-by-one errors)
- incorrect_period_comparison: Wrong logic for comparing current vs prior periods (YoY, MoM, QoQ)

**Aggregation & Granularity:**
- aggregation_mismatch: Aggregation level doesn't match question requirements
- output_granularity_mismatch: Results at wrong granularity (daily vs monthly, etc.)
- mixing_aggregated_and_non_aggregated: Invalid SQL mixing aggregate and non-aggregate columns
- incorrect_kpi_aggregation: Wrong aggregation function for KPI (SUM vs AVG, etc.)
- incorrect_ratio_calculation: Missing or wrong denominators in percentage/ratio calculations

**Filters & Conditions:**
- missing_filter: Required filters missing (region, product, date range, etc.)
- incorrect_filter_logic: Wrong filter conditions or operators
- missing_business_mandatory_filters: Business rule filters not applied
- redundant_or_contradictory_filters: Conflicting filter conditions

**Schema & Table Usage:**
- missing_table: Required tables not included in query
- incorrect_table_selection: Wrong table used for requested data
- incorrect_join_logic: Wrong join conditions or join types
- missing_dimensional_context: Missing dimension tables for requested entities

**Output Completeness:**
- missing_required_output_columns: Expected columns not in SELECT
- missing_date_output: Query doesn't return date/period columns
- missing_actual_dates_in_output: Doesn't show actual date ranges used
- output_not_labeled_by_timeframe: Results not labeled with period/year

**Function & SQL Usage:**
- use_of_prohibited_functions: Using functions not allowed by business rules
- incorrect_window_function_usage: Wrong OVER clause or partitioning logic
- invalid_sql_syntax: Non-standard or incorrect SQL syntax

**Business Rule Compliance:**
- missing_business_rule: Business rule not implemented in query
- incorrect_business_rule_implementation: Business rule applied incorrectly
- violation_of_reporting_constraints: Breaks reporting rules (e.g., store-level for finance KPIs)

**Intent & Interpretation:**
- ambiguous_user_intent: Question unclear, query makes assumptions
- incorrect_inferred_intent: Query interprets question incorrectly

**General:**
- incorrect_literal_values: Literal values that are incorrect or don't match the intended logic (distinct from correct date calculations)
- incorrect_comparison_logic: Wrong logic for comparing periods or values
- query_does_not_answer_question: Query fundamentally doesn't address the question

## Ambiguity Detection & Business Rule Tracking

**YOUR CRITICAL TASK:** Identify any ambiguities or incompleteness in the user question that required assumptions or defaults in the SQL query.

### What Constitutes Ambiguity?

An ambiguity exists when the input user question can be interpreted in multiple ways and can lead to generation of multiple
separate sql queries each of which can be potentially considered consistent with user input question. Here are some examples
of potential source of ambiguities:

1. **Missing Information**: Question lacks essential details (timeframe, filters, aggregation level)
2. **Vague Language**: Terms like "recent," "top," "high" without clear thresholds
3. **Multiple Valid Interpretations**: Question could be answered in different ways

### Common Ambiguity Patterns (Not Exhaustive)

**Temporal Ambiguity:**
- Missing timeframe (no year/month/quarter specified)
- Vague timeframe ("recent," "current," "latest" without definition)
- Unclear period boundaries for comparisons (YTD vs full period, equal vs unequal periods)

**Filtering Ambiguity:**
- Missing retailer/region/brand filters when context suggests they're important
- Unclear scope ("all stores" vs "specific region")

**Aggregation Ambiguity:**
- Unclear granularity (daily vs weekly vs monthly)
- Ambiguous "top N" without N specified

**Calculation Ambiguity:**
- Unclear KPI definitions when multiple formulas exist
- Missing calculation method (average vs sum vs median)

**Output Ambiguity:**
- Unclear column selection
- Ambiguous grouping dimensions

### Business Rule Tracking

When a business rule was applied to resolve an ambiguity:
1. **Identify rule name**: Extract from business rules context (e.g., "timeframe_defaults")
2. **Document assumption**: State what default/assumption was applied
3. **Format**: "rule_name: specific assumption made"

**IMPORTANT:** Be generous in detecting ambiguities. If the SQL query made ANY assumption not explicitly stated in the user question, mark it as ambiguous.

## Response Format:

You MUST respond with a valid JSON object in the following format:
{
    "status": "pass" or "fail",
    "confidence": <float between 0.0 and 1.0>,
    "reasoning": "<detailed explanation of your evaluation>",
    "issues": ["<issue_type>: <specific description>", ...] or [] if no issues,

    "ambiguity_status": "ambiguous" or "non-ambiguous",
    "ambiguities_detected": [
        {
            "type": "<category>",
            "description": "<what was unclear or missing in the question>",
            "resolution": "<how the SQL query resolved it>"
        },
        ...
    ] or [] if non-ambiguous,
    "applied_business_rules": [
        {
            "rule_name": "<name from business rules context>",
            "assumption": "<specific default/assumption applied>",
            "resolves_ambiguity": "<which ambiguity this rule addressed>"
        },
        ...
    ] or [] if no rules applied,
    "business_rule_coverage": "business-rule-present" or "business-rule-not-present" or "not-applicable"
}

**Field Definitions:**

- **ambiguity_status**: "ambiguous" if question lacks critical info requiring assumptions, else "non-ambiguous"
- **ambiguities_detected**: Array of ambiguity objects (empty if non-ambiguous)
- **applied_business_rules**: Array of rule applications (empty if none)
- **business_rule_coverage**:
  - "business-rule-present": Ambiguous + at least one business rule applied
  - "business-rule-not-present": Ambiguous + no business rules applied (ad-hoc assumptions)
  - "not-applicable": Non-ambiguous question (no coverage needed)

**Important:** When identifying issues, use the issue type keys from the reference guide above, followed by a colon and specific description. For example:
- "timeframe_mismatch: Query filters for January 2024 but user asked for current month (December 2024)"
- "missing_filter: Region filter not applied as required by business rules"
- "aggregation_mismatch: Query returns daily data when monthly aggregation was requested"
- "incorrect_to_date_logic: MTD calculation includes future dates beyond today"

Be strict but fair in your evaluation. A query should PASS if it would produce correct results for the given question. Minor style differences are acceptable if the logic is correct."""


def build_evaluation_prompt(question, sql_query, context):
    """Build the evaluation prompt with all context."""
    prompt = f"""## Context Information

{context}

---

## Question to Evaluate

**User Question:** {question}

**Generated SQL Query:**
```sql
{sql_query}
```

---

## Your Task

Evaluate whether the SQL query correctly answers the user question based on the provided context. Consider:
1. Is the query using the correct tables and columns?
2. Are KPI formulas applied correctly?
3. Are all business rules followed?
4. Is the aggregation level appropriate?
5. Are filter conditions correct and complete?

Provide your evaluation in the required JSON format."""

    return prompt


def parse_yaml_content(content):
    """Parse YAML content and return structured data."""
    try:
        return yaml.safe_load(content)
    except yaml.YAMLError as e:
        st.error(f"Error parsing YAML: {e}")
        return None


def parse_json_content(content):
    """Parse JSON content and return structured data."""
    try:
        return json.loads(content)
    except json.JSONDecodeError as e:
        st.error(f"Error parsing JSON: {e}")
        return None


def parse_csv_content(content):
    """Parse CSV content and return DataFrame."""
    try:
        return pd.read_csv(StringIO(content))
    except Exception as e:
        st.error(f"Error parsing CSV: {e}")
        return None


def format_business_rules_only(context_data):
    """Format only business rules into a prompt string for Component 2."""
    if not context_data:
        return "No business rules provided."

    rules = context_data.get('business_rules') if isinstance(context_data, dict) else None

    if not rules:
        return "No business rules provided."

    rules_text = "## Business Rules\n"
    if isinstance(rules, list):
        # Multiple files format: list of {'file': name, 'content': data}
        for item in rules:
            if isinstance(item, dict) and 'file' in item:
                rules_text += f"\n### {item['file']}\n"
                content = item.get('content', '')
                if isinstance(content, dict):
                    rules_text += yaml.dump(content, default_flow_style=False)
                else:
                    rules_text += str(content)
            else:
                rules_text += str(item)
    elif isinstance(rules, dict):
        rules_text += yaml.dump(rules, default_flow_style=False)
    else:
        rules_text += str(rules)

    return rules_text


def format_context_for_prompt(context_data):
    """Format all context data into a structured prompt string."""
    context_parts = []

    if context_data.get('business_rules'):
        rules = context_data['business_rules']
        rules_text = "## Business Rules\n"
        if isinstance(rules, list):
            # Multiple files format: list of {'file': name, 'content': data}
            for item in rules:
                if isinstance(item, dict) and 'file' in item:
                    rules_text += f"\n### {item['file']}\n"
                    content = item.get('content', '')
                    if isinstance(content, dict):
                        rules_text += yaml.dump(content, default_flow_style=False)
                    else:
                        rules_text += str(content)
                else:
                    rules_text += str(item)
        elif isinstance(rules, dict):
            rules_text += yaml.dump(rules, default_flow_style=False)
        else:
            rules_text += str(rules)
        context_parts.append(rules_text)

    if context_data.get('kpi_formulas'):
        formulas = context_data['kpi_formulas']
        formulas_text = "## KPI Formulas\n"
        if isinstance(formulas, list):
            # Multiple files format: list of {'file': name, 'content': data}
            for item in formulas:
                if isinstance(item, dict) and 'file' in item:
                    formulas_text += f"\n### {item['file']}\n"
                    content = item.get('content', '')
                    if isinstance(content, dict):
                        formulas_text += yaml.dump(content, default_flow_style=False)
                    else:
                        formulas_text += str(content)
                else:
                    formulas_text += str(item)
        elif isinstance(formulas, dict):
            formulas_text += yaml.dump(formulas, default_flow_style=False)
        else:
            formulas_text += str(formulas)
        context_parts.append(formulas_text)

    if context_data.get('data_dictionary'):
        ddl = context_data['data_dictionary']
        ddl_text = "## Data Dictionary (DDL)\n"
        if isinstance(ddl, list):
            # Multiple files format: list of {'file': name, 'content': data}
            for item in ddl:
                if isinstance(item, dict) and 'file' in item:
                    ddl_text += f"\n### {item['file']}\n"
                    content = item.get('content', '')
                    if isinstance(content, dict):
                        ddl_text += yaml.dump(content, default_flow_style=False)
                    elif isinstance(content, list):
                        ddl_text += "\n".join([
                            sub_item.get('ddl', str(sub_item)) if isinstance(sub_item, dict) else str(sub_item)
                            for sub_item in content
                        ])
                    else:
                        ddl_text += str(content)
                elif isinstance(item, dict) and 'ddl' in item:
                    ddl_text += item.get('ddl', str(item))
                else:
                    ddl_text += str(item)
        elif isinstance(ddl, dict):
            ddl_text += yaml.dump(ddl, default_flow_style=False)
        else:
            ddl_text += str(ddl)
        context_parts.append(ddl_text)

    if context_data.get('table_samples'):
        samples_text = "## Table Samples\n"
        for table_name, sample_df in context_data['table_samples'].items():
            if sample_df is not None:
                samples_text += f"\n### {table_name}\n"
                samples_text += sample_df.to_markdown(index=False) + "\n"
        context_parts.append(samples_text)

    return "\n\n".join(context_parts)


def preprocess_evaluation_results(evaluation_results):
    """
    Preprocess evaluation results for summary report generation.

    Args:
        evaluation_results: List of evaluation result dicts from st.session_state

    Returns:
        dict with statistics and top 10 issues
    """
    if not evaluation_results:
        raise ValueError("No evaluation results to process")

    total_queries = len(evaluation_results)
    passed_count = sum(1 for r in evaluation_results if r.get('status') == 'pass')
    failed_count = sum(1 for r in evaluation_results if r.get('status') == 'fail')
    error_count = sum(1 for r in evaluation_results if r.get('status') == 'error')
    pass_rate = (passed_count / total_queries * 100) if total_queries > 0 else 0

    # Calculate average confidence
    confidences = [r.get('confidence', 0.0) for r in evaluation_results]
    avg_confidence = sum(confidences) / len(confidences) if confidences else 0.0

    # Calculate confidence by status
    confidence_by_status = {}
    for status in ['pass', 'fail', 'error']:
        status_confidences = [r.get('confidence', 0.0) for r in evaluation_results if r.get('status') == status]
        confidence_by_status[status] = sum(status_confidences) / len(status_confidences) if status_confidences else 0.0

    # Flatten all issues from the issues lists and extract issue keys
    all_issues = []
    for result in evaluation_results:
        issues = result.get('issues', [])
        if isinstance(issues, list):
            for issue in issues:
                # Extract key from issue (before ':' if present, otherwise use full string)
                issue_str = str(issue).strip()
                issue_key = issue_str.split(':')[0].strip() if ':' in issue_str else issue_str
                all_issues.append(issue_key)
        elif issues:  # If it's a single string
            issue_str = str(issues).strip()
            issue_key = issue_str.split(':')[0].strip() if ':' in issue_str else issue_str
            all_issues.append(issue_key)

    # Count issue frequencies
    issue_counter = Counter(all_issues)
    top_10 = issue_counter.most_common(10)
    total_issues = len(all_issues)

    # Calculate percentages for top 10 issues based on total queries
    top_10_issues = []
    for issue, count in top_10:
        # Percentage = queries with this issue / total queries * 100
        percentage = (count / total_queries * 100) if total_queries > 0 else 0
        top_10_issues.append({
            'issue': str(issue),
            'count': count,
            'percentage': percentage
        })

    # Calculate failed queries count and average issues per failed
    failed_queries_count = sum(1 for r in evaluation_results if r.get('issues') and len(r.get('issues', [])) > 0)
    avg_issues_per_failed = (total_issues / failed_queries_count) if failed_queries_count > 0 else 0

    # Collect detailed information from failed queries for LLM analysis
    failed_queries_details = []
    for result in evaluation_results:
        if result.get('status') == 'fail':
            failed_queries_details.append({
                'question': result.get('question', ''),
                'issues': result.get('issues', []),
                'reasoning': result.get('reasoning', ''),
                'confidence': result.get('confidence', 0.0)
            })

    # NEW: Calculate ambiguity metrics
    ambiguous_count = sum(1 for r in evaluation_results if r.get('ambiguity_status') == 'ambiguous')
    ambiguous_rate = (ambiguous_count / total_queries * 100) if total_queries > 0 else 0

    # Count by combined status - ALL COMBINATIONS
    # Non-Ambiguous combinations (coverage is not-applicable)
    pass_non_ambiguous = sum(1 for r in evaluation_results
                              if r.get('status') == 'pass' and r.get('ambiguity_status') == 'non-ambiguous')
    fail_non_ambiguous = sum(1 for r in evaluation_results
                              if r.get('status') == 'fail' and r.get('ambiguity_status') == 'non-ambiguous')
    error_non_ambiguous = sum(1 for r in evaluation_results
                               if r.get('status') == 'error' and r.get('ambiguity_status') == 'non-ambiguous')

    # Ambiguous + Fully Covered combinations
    pass_ambiguous_fully_covered = sum(1 for r in evaluation_results
                                        if r.get('status') == 'pass'
                                        and r.get('ambiguity_status') == 'ambiguous'
                                        and r.get('business_rule_coverage') == 'business-rule-present')
    fail_ambiguous_fully_covered = sum(1 for r in evaluation_results
                                        if r.get('status') == 'fail'
                                        and r.get('ambiguity_status') == 'ambiguous'
                                        and r.get('business_rule_coverage') == 'business-rule-present')
    error_ambiguous_fully_covered = sum(1 for r in evaluation_results
                                         if r.get('status') == 'error'
                                         and r.get('ambiguity_status') == 'ambiguous'
                                         and r.get('business_rule_coverage') == 'business-rule-present')

    # Ambiguous + Partially Covered combinations
    pass_ambiguous_partial = sum(1 for r in evaluation_results
                                  if r.get('status') == 'pass'
                                  and r.get('ambiguity_status') == 'ambiguous'
                                  and r.get('business_rule_coverage') == 'partially-covered')
    fail_ambiguous_partial = sum(1 for r in evaluation_results
                                  if r.get('status') == 'fail'
                                  and r.get('ambiguity_status') == 'ambiguous'
                                  and r.get('business_rule_coverage') == 'partially-covered')
    error_ambiguous_partial = sum(1 for r in evaluation_results
                                   if r.get('status') == 'error'
                                   and r.get('ambiguity_status') == 'ambiguous'
                                   and r.get('business_rule_coverage') == 'partially-covered')

    # Ambiguous + Not Covered combinations
    pass_ambiguous_no_rule = sum(1 for r in evaluation_results
                                  if r.get('status') == 'pass'
                                  and r.get('ambiguity_status') == 'ambiguous'
                                  and r.get('business_rule_coverage') == 'business-rule-not-present')
    fail_ambiguous_no_rule = sum(1 for r in evaluation_results
                                  if r.get('status') == 'fail'
                                  and r.get('ambiguity_status') == 'ambiguous'
                                  and r.get('business_rule_coverage') == 'business-rule-not-present')
    error_ambiguous_no_rule = sum(1 for r in evaluation_results
                                   if r.get('status') == 'error'
                                   and r.get('ambiguity_status') == 'ambiguous'
                                   and r.get('business_rule_coverage') == 'business-rule-not-present')

    # For backward compatibility
    pass_ambiguous_rule_present = pass_ambiguous_fully_covered

    # Collect ambiguity types
    all_ambiguity_types = []
    for result in evaluation_results:
        ambiguities = result.get('ambiguities_detected', [])
        if isinstance(ambiguities, list):
            for amb in ambiguities:
                if isinstance(amb, dict) and 'type' in amb:
                    all_ambiguity_types.append(amb['type'])

    ambiguity_type_counter = Counter(all_ambiguity_types)
    top_10_ambiguity_types = ambiguity_type_counter.most_common(10)

    # Calculate rule coverage
    rule_present_count = sum(1 for r in evaluation_results
                              if r.get('business_rule_coverage') == 'business-rule-present')
    rule_coverage_rate = (rule_present_count / ambiguous_count * 100) if ambiguous_count > 0 else 0

    # Collect applied rules
    all_applied_rules = []
    for result in evaluation_results:
        rules = result.get('applied_business_rules', [])
        if isinstance(rules, list):
            for rule in rules:
                if isinstance(rule, dict) and 'rule_name' in rule:
                    all_applied_rules.append(rule['rule_name'])

    rule_usage_counter = Counter(all_applied_rules)
    top_10_rules = rule_usage_counter.most_common(10)

    return {
        'total_queries': total_queries,
        'passed_count': passed_count,
        'failed_count': failed_count,
        'error_count': error_count,
        'pass_rate': pass_rate,
        'avg_confidence': avg_confidence,
        'confidence_by_status': confidence_by_status,
        'top_10_issues': top_10_issues,
        'failed_queries_count': failed_queries_count,
        'avg_issues_per_failed': avg_issues_per_failed,
        'total_issues': total_issues,
        'failed_queries_details': failed_queries_details,

        # NEW: Ambiguity metrics
        'ambiguous_count': ambiguous_count,
        'ambiguous_rate': ambiguous_rate,
        # Non-Ambiguous combinations
        'pass_non_ambiguous': pass_non_ambiguous,
        'fail_non_ambiguous': fail_non_ambiguous,
        'error_non_ambiguous': error_non_ambiguous,
        # Ambiguous + Fully Covered
        'pass_ambiguous_fully_covered': pass_ambiguous_fully_covered,
        'fail_ambiguous_fully_covered': fail_ambiguous_fully_covered,
        'error_ambiguous_fully_covered': error_ambiguous_fully_covered,
        # Ambiguous + Partially Covered
        'pass_ambiguous_partial': pass_ambiguous_partial,
        'fail_ambiguous_partial': fail_ambiguous_partial,
        'error_ambiguous_partial': error_ambiguous_partial,
        # Ambiguous + Not Covered
        'pass_ambiguous_no_rule': pass_ambiguous_no_rule,
        'fail_ambiguous_no_rule': fail_ambiguous_no_rule,
        'error_ambiguous_no_rule': error_ambiguous_no_rule,
        # Backward compatibility
        'pass_ambiguous_rule_present': pass_ambiguous_rule_present,
        'top_10_ambiguity_types': [
            {'type': amb_type, 'count': count, 'percentage': (count / ambiguous_count * 100) if ambiguous_count > 0 else 0}
            for amb_type, count in top_10_ambiguity_types
        ],
        'rule_present_count': rule_present_count,
        'rule_coverage_rate': rule_coverage_rate,
        'top_10_applied_rules': [
            {'rule': rule_name, 'count': count, 'percentage': (count / total_queries * 100) if total_queries > 0 else 0}
            for rule_name, count in top_10_rules
        ]
    }


def format_ambiguities_for_display(ambiguities_detected):
    """Format ambiguities array into human-readable string."""
    if not ambiguities_detected:
        return "None"

    formatted = []
    for amb in ambiguities_detected:
        if isinstance(amb, dict):
            type_str = amb.get('type', 'unknown')
            desc_str = amb.get('description', 'No description')
            formatted.append(f"‚Ä¢ [{type_str}] {desc_str}")
        else:
            formatted.append(f"‚Ä¢ {str(amb)}")

    return "\n".join(formatted)


def format_rules_for_display(applied_business_rules):
    """Format applied rules array into human-readable string."""
    if not applied_business_rules:
        return "None"

    formatted = []
    for rule in applied_business_rules:
        if isinstance(rule, dict):
            rule_name = rule.get('rule_name', 'unknown')
            assumption = rule.get('assumption', 'No assumption')
            formatted.append(f"‚Ä¢ {rule_name}: {assumption}")
        else:
            formatted.append(f"‚Ä¢ {str(rule)}")

    return "\n".join(formatted)


def generate_llm_summary(preprocessed_data, api_key, model, api_provider="OpenAI",
                        azure_base_url=None, azure_api_version=None):
    """
    Generate comprehensive LLM summary of evaluation results.

    Args:
        preprocessed_data: Output from preprocess_evaluation_results()
        api_key: OpenAI or Azure OpenAI API key
        model: Model name/deployment name
        api_provider: "OpenAI" or "Azure OpenAI"
        azure_base_url: Azure endpoint (if using Azure)
        azure_api_version: Azure API version (if using Azure)

    Returns:
        str - markdown-formatted LLM summary

    Raises:
        Exception: API errors, timeouts, or invalid responses
    """
    try:
        # Create OpenAI client based on provider
        if api_provider == "Azure OpenAI":
            client = AzureOpenAI(
                api_key=api_key,
                azure_endpoint=azure_base_url,
                api_version=azure_api_version
            )
        else:
            client = OpenAI(api_key=api_key)

        # Build issues table for prompt
        issues_table = "| Issue | Count | % of Queries |\n|-------|-------|------------|\n"
        if preprocessed_data['top_10_issues']:
            for issue_data in preprocessed_data['top_10_issues']:
                issues_table += f"| {issue_data['issue']} | {issue_data['count']} | {issue_data['percentage']:.1f}% |\n"
        else:
            issues_table += "| No issues found | 0 | 0% |\n"

        # Build failed queries details section
        failed_details_section = ""
        if preprocessed_data['failed_queries_details']:
            failed_details_section = "\n\nFAILED QUERIES WITH DETAILED REASONING:\n"
            for idx, query_detail in enumerate(preprocessed_data['failed_queries_details'], 1):
                # Pass complete issues (not just keys) for detailed analysis
                if query_detail['issues']:
                    issues_str = "\n      * " + "\n      * ".join([str(issue) for issue in query_detail['issues']])
                else:
                    issues_str = "No issues"
                failed_details_section += f"\n{idx}. Question: {query_detail['question'][:150]}{'...' if len(query_detail['question']) > 150 else ''}\n"
                failed_details_section += f"   Issues: {issues_str}\n"
                failed_details_section += f"   Reasoning: {query_detail['reasoning'][:500]}{'...' if len(query_detail['reasoning']) > 500 else ''}\n"
                failed_details_section += f"   Confidence: {query_detail['confidence']:.2f}\n"

        # Build user prompt
        user_prompt = f"""You are analyzing SQL query evaluation results for a chatbot-generated query system.

STATISTICS:
- Total Queries: {preprocessed_data['total_queries']}
- Pass Rate: {preprocessed_data['pass_rate']:.1f}%
- Passed: {preprocessed_data['passed_count']}, Failed: {preprocessed_data['failed_count']}, Errors: {preprocessed_data['error_count']}
- Average Confidence: {preprocessed_data['avg_confidence']:.2f}
- Queries with Issues: {preprocessed_data['failed_queries_count']}
- Average Issues per Failed Query: {preprocessed_data['avg_issues_per_failed']:.1f}

MOST COMMON ISSUES:
{issues_table}{failed_details_section}

TASK:
Provide a comprehensive business-focused analysis covering:
1. **Overall Quality Assessment**: Is the SQL generation system performing well? What's the confidence level?
2. **Key Patterns in Failures**: What types of queries fail most often? Are there systematic issues? Look at the actual reasoning to identify deeper patterns.
3. **Top Improvement Areas**: Based on the failed query details, prioritize the top 3-5 concrete recommendations. Format each as: **Bold Title** followed by explanation text. Do NOT use numbered lists - use paragraphs with bold headings instead.
4. **Business Impact**: What's the real-world impact of these failure patterns on business decisions? What could go wrong if these queries are used?
5. **Root Cause Analysis**: Based on the reasoning provided, what are the underlying root causes? Are these chatbot training issues, business rule gaps, or data understanding problems?

Provide detailed insights (400-600 words). Use markdown formatting with #### (H4) headers for each section to keep them appropriately sized. For the Top Improvement Areas section specifically, use **bold text** for recommendation titles followed by regular paragraph text explaining each recommendation. Be specific and reference patterns you see in the failed query details."""

        # Call OpenAI API
        response = client.chat.completions.create(
            model=model,
            messages=[
                {"role": "system", "content": "You are a SQL evaluation analyst providing deep insights on SQL query quality, patterns, and root causes. You excel at identifying systematic issues and providing actionable recommendations."},
                {"role": "user", "content": user_prompt}
            ],
            temperature=0.7,
            max_tokens=1500
        )

        return response.choices[0].message.content

    except Exception as e:
        raise Exception(f"Failed to generate LLM summary: {str(e)}")


def format_pdf_report_markdown(preprocessed_data, llm_summary):
    """
    Format complete markdown report with statistics, issues, and summary.

    Args:
        preprocessed_data: Output from preprocess_evaluation_results()
        llm_summary: Output from generate_llm_summary()

    Returns:
        str - complete markdown report
    """
    timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")

    # Determine overall status
    pass_rate = preprocessed_data['pass_rate']
    if pass_rate >= 90:
        status_indicator = "Excellent"
    elif pass_rate >= 75:
        status_indicator = "Good"
    elif pass_rate >= 60:
        status_indicator = "Needs Improvement"
    else:
        status_indicator = "Critical"

    # Start building markdown with enhanced header
    markdown_content = f"""## SQL Query Evaluation Report

**Generated:** {timestamp}
**Overall Status:** {status_indicator} ({pass_rate:.1f}% Pass Rate)

---

### Executive Summary

This report analyzes {preprocessed_data['total_queries']} SQL queries generated by the chatbot system. The evaluation assessed query correctness, adherence to business rules, and data accuracy.

**Performance Overview:**

| Metric | Value |
|--------|-------|
| Total Queries Evaluated | {preprocessed_data['total_queries']} |
| Passed Queries | {preprocessed_data['passed_count']} ({(preprocessed_data['passed_count']/preprocessed_data['total_queries']*100):.1f}%) |
| Failed Queries | {preprocessed_data['failed_count']} ({(preprocessed_data['failed_count']/preprocessed_data['total_queries']*100):.1f}%) |
| Error Queries | {preprocessed_data['error_count']} ({(preprocessed_data['error_count']/preprocessed_data['total_queries']*100):.1f}%) |
| Average Confidence Score | {preprocessed_data['avg_confidence']:.2f} / 1.00 |
| Total Issues Identified | {preprocessed_data['total_issues']} |
| Average Issues per Failed Query | {preprocessed_data['avg_issues_per_failed']:.1f} |

---

### Issue Distribution

The following table shows the most frequently occurring issues and the percentage of queries affected by each issue:

"""

    # Add issues table with enhanced formatting
    if preprocessed_data['top_10_issues']:
        markdown_content += "| Issue Type | Occurrences | % of Queries |\n"
        markdown_content += "|------------|-------------|------------|\n"
        for issue_data in preprocessed_data['top_10_issues']:
            markdown_content += f"| {issue_data['issue']} | {issue_data['count']} | {issue_data['percentage']:.1f}% |\n"
    else:
        markdown_content += "*No issues found - all queries passed successfully!*\n"

    # NEW: Add ambiguity analysis section
    markdown_content += f"""

---

### Ambiguity Analysis

The evaluation identified ambiguities in user questions that required assumptions or defaults.

**Ambiguity Overview:**

| Metric | Value |
|--------|-------|
| Ambiguous Queries | {preprocessed_data['ambiguous_count']} ({preprocessed_data['ambiguous_rate']:.1f}%) |
| Rule Coverage | {preprocessed_data['rule_present_count']} ({preprocessed_data['rule_coverage_rate']:.1f}%) |
| Pass (Non-Ambiguous) | {preprocessed_data['pass_non_ambiguous']} |
| Pass (Ambiguous + Rule) | {preprocessed_data['pass_ambiguous_rule_present']} |
| Fail (Ambiguous + No Rule) | {preprocessed_data['fail_ambiguous_no_rule']} |
| Fail (Non-Ambiguous) | {preprocessed_data['fail_non_ambiguous']} |

**Most Common Ambiguity Types:**

"""

    if preprocessed_data['top_10_ambiguity_types']:
        markdown_content += "| Ambiguity Type | Occurrences | % of Ambiguous |\n"
        markdown_content += "|----------------|-------------|----------------|\n"
        for item in preprocessed_data['top_10_ambiguity_types']:
            markdown_content += f"| {item['type']} | {item['count']} | {item['percentage']:.1f}% |\n"
    else:
        markdown_content += "*No ambiguities detected*\n"

    markdown_content += "\n**Most Applied Business Rules:**\n\n"

    if preprocessed_data['top_10_applied_rules']:
        markdown_content += "| Business Rule | Times Applied | % of Queries |\n"
        markdown_content += "|---------------|---------------|-------------|\n"
        for item in preprocessed_data['top_10_applied_rules']:
            markdown_content += f"| {item['rule']} | {item['count']} | {item['percentage']:.1f}% |\n"
    else:
        markdown_content += "*No business rules were applied*\n"

    # Add key insights section before detailed analysis
    markdown_content += f"""

---

### Key Insights

- **Pass Rate:** {preprocessed_data['pass_rate']:.1f}% of queries meet all evaluation criteria
- **Confidence Level:** Average confidence score is {preprocessed_data['avg_confidence']:.2f}, indicating {"high" if preprocessed_data['avg_confidence'] >= 0.7 else "moderate" if preprocessed_data['avg_confidence'] >= 0.5 else "low"} reliability
- **Issue Severity:** {preprocessed_data['failed_queries_count']} queries have issues requiring attention
- **Top Issue:** {f"{preprocessed_data['top_10_issues'][0]['issue']} (affects {preprocessed_data['top_10_issues'][0]['percentage']:.1f}% of queries) is the primary concern" if preprocessed_data['top_10_issues'] else "None - all queries passed"}

---

### Detailed Analysis & Recommendations

{llm_summary}
"""

    return markdown_content


def generate_evaluation_pdf_report(evaluation_results, api_key, model, api_provider="OpenAI",
                                   azure_base_url=None, azure_api_version=None):
    """
    Main orchestration function: preprocess ‚Üí LLM ‚Üí format ‚Üí PDF.

    Args:
        evaluation_results: List of evaluation results
        api_key: OpenAI/Azure API key
        model: Model name
        api_provider: Provider type
        azure_base_url: Azure endpoint (if applicable)
        azure_api_version: Azure API version (if applicable)

    Returns:
        bytes - PDF file content ready for download

    Raises:
        ValueError: Empty or invalid results
        Exception: API failures, PDF generation failures
    """
    try:
        # Step 1: Preprocess evaluation results
        preprocessed_data = preprocess_evaluation_results(evaluation_results)

        # Step 2: Generate LLM summary
        llm_summary = generate_llm_summary(
            preprocessed_data,
            api_key,
            model,
            api_provider,
            azure_base_url,
            azure_api_version
        )

        # Step 3: Format markdown report
        markdown_report = format_pdf_report_markdown(preprocessed_data, llm_summary)

        # Step 4: Convert to PDF
        pdf_bytes = markdown_to_pdf_bytes(markdown_report)

        return pdf_bytes

    except ValueError as e:
        raise ValueError(f"Invalid evaluation results: {str(e)}")
    except Exception as e:
        raise Exception(f"Failed to generate PDF report: {str(e)}")


def evaluate_single_query_openai(question, sql_query, context, api_key, model="gpt-4",
                                  api_provider="OpenAI", azure_base_url=None, azure_api_version=None):
    """
    Evaluate a single question-SQL pair using OpenAI API or Azure OpenAI.
    Returns evaluation result with pass/fail status and reasoning.
    """
    try:
        # Create OpenAI client based on provider
        if api_provider == "Azure OpenAI":
            client = AzureOpenAI(
                api_key=api_key,
                azure_endpoint=azure_base_url,
                api_version=azure_api_version
            )
        else:
            client = OpenAI(api_key=api_key)

        evaluation_prompt = build_evaluation_prompt(question, sql_query, context)

        response = client.chat.completions.create(
            model=model,
            messages=[
                {"role": "system", "content": EVALUATION_SYSTEM_PROMPT},
                {"role": "user", "content": evaluation_prompt}
            ],
            temperature=0.1,  # Low temperature for consistent evaluation
            max_tokens=1000
        )

        response_text = response.choices[0].message.content.strip()

        # Parse JSON response
        # Try to extract JSON from the response (handle markdown code blocks)
        if "```json" in response_text:
            json_start = response_text.find("```json") + 7
            json_end = response_text.find("```", json_start)
            response_text = response_text[json_start:json_end].strip()
        elif "```" in response_text:
            json_start = response_text.find("```") + 3
            json_end = response_text.find("```", json_start)
            response_text = response_text[json_start:json_end].strip()

        try:
            result = json.loads(response_text)
        except json.JSONDecodeError:
            # Try to find JSON object in the response
            import re
            json_match = re.search(r'\{[^{}]*\}', response_text, re.DOTALL)
            if json_match:
                result = json.loads(json_match.group())
            else:
                # Fallback: create a result from the text
                result = {
                    "status": "fail",
                    "confidence": 0.5,
                    "reasoning": f"Could not parse LLM response: {response_text[:500]}",
                    "issues": ["Failed to parse evaluation response"]
                }

        return {
            'question': question,
            'sql_query': sql_query,
            'status': result.get('status', 'fail').lower(),
            'reasoning': result.get('reasoning', 'No reasoning provided'),
            'issues': result.get('issues', []),
            'confidence': float(result.get('confidence', 0.5)),

            # NEW: Ambiguity tracking
            'ambiguity_status': result.get('ambiguity_status', 'non-ambiguous'),
            'ambiguities_detected': result.get('ambiguities_detected', []),
            'applied_business_rules': result.get('applied_business_rules', []),
            'business_rule_coverage': result.get('business_rule_coverage', 'not-applicable')
        }

    except Exception as e:
        return {
            'question': question,
            'sql_query': sql_query,
            'status': 'error',
            'reasoning': f"API Error: {str(e)}",
            'issues': [f"Error during evaluation: {str(e)}"],
            'confidence': 0.0,
            # Add defaults for new fields
            'ambiguity_status': 'non-ambiguous',
            'ambiguities_detected': [],
            'applied_business_rules': [],
            'business_rule_coverage': 'not-applicable'
        }


# =============================================================================
# UPDATED EVALUATOR - COMPONENT PROMPTS
# =============================================================================

COMPONENT1_AMBIGUITY_DETECTION_PROMPT = """You are an expert Question Ambiguity Analyzer. Your task is to analyze user questions and determine if they are ambiguous or non-ambiguous for SQL query generation.

## What Constitutes Ambiguity?

An ambiguity exists when the input user question can be interpreted in multiple ways and can lead to generation of multiple separate SQL queries, each of which can be potentially considered consistent with the user input question.

Here are some examples of potential sources of ambiguities:

1. **Missing Information**: Question lacks essential details (timeframe, filters, aggregation level)
2. **Vague Language**: Terms like "recent," "top," "high" without clear thresholds
3. **Multiple Valid Interpretations**: Question could be answered in different ways

### Common Ambiguity Patterns (Not Exhaustive)

**Temporal Ambiguity:**
- Missing timeframe (no year/month/quarter specified): "Show me sales" (which period?)
- Vague timeframe ("recent," "current," "latest" without definition): "recent sales" (last week? last month?)
- Unclear period boundaries: "Q1 sales" (which year?)
- Ambiguous comparisons for YTD vs full period, equal vs unequal periods: "sales growth" (compared to what?)

**Filtering Ambiguity:**
- Missing retailer/region/brand filters when context suggests they're important
- Unclear scope ("all stores" vs "specific region"): "store performance" (which stores?)
- Missing constraints: "customer orders" (all customers? specific segment?)

**Aggregation Ambiguity:**
- Unclear granularity (daily vs weekly vs monthly): "sales trend" (at what level?)
- Ambiguous "top N" without N specified: "top selling products" (top 5? top 10? top 100?)
- Unclear grouping: "sales by region" (also by time? by product?)

**Calculation Ambiguity:**
- Unclear KPI definitions when multiple formulas exist: "conversion rate" (which formula?)
- Missing calculation method (average vs sum vs median): "average order value" (mean? median?)
- Multiple possible metrics: "performance" (revenue? units? margin?)

**Output Ambiguity:**
- Unclear column selection: "show me product data" (which columns?)
- Ambiguous grouping dimensions: "breakdown by category" (also by time?)
- Missing sort order: "list customers" (sorted by what?)

## What is NOT Ambiguous?

A question is NON-AMBIGUOUS when it can only lead to ONE valid SQL query interpretation:

- Questions with explicit timeframes: "sales for January 2024"
- Questions with clear filters: "sales in North region for Q1 2024"
- Questions with specific metrics: "total revenue by store for last month"
- Questions with defined scope: "top 10 products by units sold in 2024"
- Questions with clear aggregation: "monthly sales totals for 2024"

## Response Format

You MUST respond with a valid JSON object:

If AMBIGUOUS:
{
    "ambiguity_status": "ambiguous",
    "interpretations": [
        {
            "id": 1,
            "interpretation": "Clear description of first valid interpretation that would lead to a specific SQL query",
            "ambiguity_type": "temporal|filtering|aggregation|calculation|output",
            "missing_information": "What specific information is missing that causes this ambiguity"
        },
        {
            "id": 2,
            "interpretation": "Clear description of second valid interpretation that would lead to a different SQL query",
            "ambiguity_type": "...",
            "missing_information": "..."
        }
    ],
    "reasoning": "Explanation of why multiple valid SQL interpretations exist"
}

If NON-AMBIGUOUS:
{
    "ambiguity_status": "non-ambiguous",
    "interpretations": [
        {
            "id": 1,
            "interpretation": "The single clear interpretation",
            "ambiguity_type": null,
            "missing_information": null
        }
    ],
    "reasoning": "Explanation of why only one SQL interpretation is valid"
}

**IMPORTANT:** Be thorough in detecting ambiguities. If the question can lead to multiple different SQL queries that are all valid interpretations of the user's intent, mark it as ambiguous."""


COMPONENT2_BUSINESS_COVERAGE_PROMPT = """You are an expert Business Rule Analyzer. Your task is to determine if the provided business rules can resolve detected ambiguities in a user question.

## Your Task

Given:
1. A user question
2. Detected ambiguities and their possible interpretations
3. Business rules document and other context

Determine if the business rules provide guidance to resolve each ambiguity.

## Coverage Levels

### fully-covered
ALL ambiguities have corresponding business rules that specify defaults or requirements.
Result: Only ONE valid interpretation remains.

### partially-covered
SOME ambiguities are resolved by business rules, but others remain open.
Result: Still MULTIPLE valid interpretations remain.

### not-covered
NO business rules address the detected ambiguities.
Result: ALL original interpretations remain valid.

## What Qualifies as a Resolving Business Rule?

A business rule resolves an ambiguity if it:
- Specifies a DEFAULT value (e.g., "default timeframe is current month")
- REQUIRES a specific filter (e.g., "always filter by retailer")
- DEFINES a calculation method (e.g., "growth rate means YoY comparison")
- MANDATES a scope (e.g., "reports are always at store level")

## Response Format

{
    "coverage_status": "fully-covered" | "partially-covered" | "not-covered",
    "resolved_interpretation": {
        "id": <interpretation_id that rules support>,
        "interpretation": "Description of the resolved interpretation",
        "supporting_rules": ["rule names that support this"]
    } | null,
    "unresolved_ambiguities": [
        {
            "ambiguity_type": "type",
            "description": "Why this remains unresolved",
            "valid_interpretation_ids": [list of still-valid interpretation IDs]
        }
    ],
    "applied_rules": [
        {
            "rule_name": "Exact name/identifier of the rule",
            "rule_content": "The actual rule text or summary",
            "resolves": "Which ambiguity this addresses"
        }
    ],
    "remaining_valid_interpretations": [
        {
            "id": <interpretation_id>,
            "interpretation": "Description"
        }
    ],
    "reasoning": "Detailed explanation of coverage analysis"
}"""


COMPONENT3_REGULAR_EVALUATOR_PROMPT = """You are an expert SQL Query Evaluator. Your task is to evaluate whether a generated SQL query correctly answers the given user question.

## Context
This evaluation is for a NON-AMBIGUOUS case - there is exactly ONE correct interpretation of the user question. The SQL query must match this interpretation precisely.

## Evaluation Criteria

1. **Correctness**: Does the SQL query correctly answer the user's question?
2. **Table Selection**: Are the correct tables being used based on the data dictionary?
3. **Formula Accuracy**: Are KPI calculations using the correct formulas?
4. **Business Rule Compliance**: Does the query follow all business rules?
5. **Column Usage**: Are the correct columns being selected and filtered?
6. **Aggregation Logic**: Is the aggregation level appropriate?
7. **Join Logic**: Are table joins correct and necessary?
8. **Filter Conditions**: Are WHERE clauses appropriate and complete?

## Common Issue Types (Reference Guide)

When evaluating queries, look for these common issue patterns (but not limited to these):

**Time & Date Logic:**
- timeframe_mismatch: Query uses wrong timeframe that doesn't match user intent
- incorrect_to_date_logic: Wrong MTD/QTD/YTD calculation or period boundaries
- incomplete_period_coverage: Partial months/quarters/weeks when full period expected
- ambiguous_date_column: Wrong date column used for filtering or aggregation
- incorrect_date_boundaries: Wrong date range boundaries (exclusive vs inclusive, off-by-one)
- incorrect_period_comparison: Wrong logic for comparing current vs prior periods (YoY, MoM, QoQ)

**Aggregation & Granularity:**
- aggregation_mismatch: Aggregation level doesn't match question requirements
- output_granularity_mismatch: Results at wrong granularity (daily vs monthly, etc.)
- mixing_aggregated_and_non_aggregated: Invalid SQL mixing aggregate and non-aggregate columns
- incorrect_kpi_aggregation: Wrong aggregation function for KPI (SUM vs AVG, etc.)
- incorrect_ratio_calculation: Missing or wrong denominators in percentage/ratio calculations

**Filters & Conditions:**
- missing_filter: Required filters missing (region, product, date range, etc.)
- incorrect_filter_logic: Wrong filter conditions or operators
- missing_business_mandatory_filters: Business rule filters not applied
- redundant_or_contradictory_filters: Conflicting filter conditions

**Schema & Table Usage:**
- missing_table: Required tables not included in query
- incorrect_table_selection: Wrong table used for requested data
- incorrect_join_logic: Wrong join conditions or join types
- missing_dimensional_context: Missing dimension tables for requested entities

**Output Completeness:**
- missing_required_output_columns: Expected columns not in SELECT
- missing_date_output: Query doesn't return date/period columns
- output_not_labeled_by_timeframe: Results not labeled with period/year

**Business Rule Compliance:**
- missing_business_rule: Business rule not implemented in query
- incorrect_business_rule_implementation: Business rule applied incorrectly
- violation_of_reporting_constraints: Breaks reporting rules

**General:**
- incorrect_literal_values: Literal values that are incorrect
- incorrect_comparison_logic: Wrong logic for comparing periods or values
- query_does_not_answer_question: Query fundamentally doesn't address the question

## Response Format

{
    "status": "pass" | "fail",
    "confidence": <float 0.0-1.0>,
    "reasoning": "Detailed explanation of evaluation",
    "issues": ["issue_type: specific description", ...] | [],
    "expected_interpretation": "What the SQL should implement",
    "actual_implementation": "What the SQL actually does"
}

Be STRICT - the SQL must correctly implement the single valid interpretation."""


COMPONENT4_AMBIGUITY_EVALUATOR_PROMPT = """You are an expert SQL Query Evaluator for AMBIGUOUS cases. Your task is to evaluate whether a generated SQL query correctly answers ANY ONE of the valid interpretations of an ambiguous user question.

## Context
This evaluation is for an AMBIGUOUS case - there are MULTIPLE valid interpretations of the user question. The SQL query should PASS if it correctly implements ANY ONE of these valid interpretations.

## Key Principle: LENIENT Evaluation

Unlike regular evaluation, here you should:
- Accept the SQL if it matches ANY valid interpretation
- Not penalize for choosing one valid interpretation over another
- Only FAIL if the SQL doesn't match ANY of the provided valid interpretations

## Evaluation Process

1. For EACH valid interpretation:
   - Check if the SQL query implements that interpretation
   - Document whether it matches and why

2. Final Decision:
   - PASS if SQL matches at least ONE valid interpretation
   - FAIL only if SQL matches NONE of the valid interpretations

## What Still Counts as Issues (even in lenient mode)

Even when matching a valid interpretation, flag these issues:

**Always Invalid (regardless of interpretation):**
- invalid_sql_syntax: Non-standard or incorrect SQL syntax
- incorrect_join_logic: Wrong join conditions or join types
- mixing_aggregated_and_non_aggregated: Invalid SQL mixing aggregate and non-aggregate columns
- use_of_prohibited_functions: Using functions not allowed by business rules

**Schema & Table Errors:**
- missing_table: Required tables not included in query
- incorrect_table_selection: Wrong table used for requested data

**Business Rule Violations (apply to ALL interpretations):**
- missing_business_mandatory_filters: Business rule filters not applied
- violation_of_reporting_constraints: Breaks reporting rules
- incorrect_business_rule_implementation: Business rule applied incorrectly

**Calculation Errors:**
- incorrect_ratio_calculation: Missing or wrong denominators
- incorrect_kpi_aggregation: Wrong aggregation function for KPI

## Response Format

{
    "status": "pass" | "fail",
    "confidence": <float 0.0-1.0>,
    "reasoning": "Detailed explanation",
    "matched_interpretation": {
        "id": <interpretation_id>,
        "interpretation": "Description of matched interpretation",
        "match_quality": "exact" | "partial"
    } | null,
    "issues": ["issue_type: description", ...] | [],
    "interpretation_analysis": [
        {
            "interpretation_id": 1,
            "matches": true | false,
            "explanation": "Detailed explanation of match/mismatch"
        }
    ]
}

Remember: PASS if ANY interpretation matches. Be lenient but thorough."""


# =============================================================================
# UPDATED EVALUATOR - COMPONENT FUNCTIONS
# =============================================================================

def create_llm_client(api_key, api_provider, azure_base_url=None, azure_api_version=None):
    """Create and return the appropriate LLM client based on provider."""
    if api_provider == "Azure OpenAI":
        return AzureOpenAI(
            api_key=api_key,
            azure_endpoint=azure_base_url,
            api_version=azure_api_version
        )
    else:
        return OpenAI(api_key=api_key)


def parse_llm_json_response(response_text):
    """Parse JSON from LLM response, handling markdown code blocks."""
    text = response_text.strip()

    # Handle markdown code blocks
    if "```json" in text:
        json_start = text.find("```json") + 7
        json_end = text.find("```", json_start)
        text = text[json_start:json_end].strip()
    elif "```" in text:
        json_start = text.find("```") + 3
        json_end = text.find("```", json_start)
        text = text[json_start:json_end].strip()

    try:
        return json.loads(text)
    except json.JSONDecodeError:
        # Try to find JSON object in the response
        import re
        json_match = re.search(r'\{[\s\S]*\}', text)
        if json_match:
            try:
                return json.loads(json_match.group())
            except json.JSONDecodeError:
                pass
        return None


def component1_ambiguity_detection(question, client, model):
    """
    Component 1: Analyze user question for ambiguity.
    Input: Question only
    Output: ambiguity_status, interpretations
    """
    try:
        user_prompt = f"""Analyze the following user question for ambiguity:

**User Question:** {question}

Determine if this question is ambiguous or non-ambiguous for SQL query generation.

If ambiguous, list ALL possible valid interpretations that could lead to different SQL queries.
If non-ambiguous, describe the single clear interpretation."""

        response = client.chat.completions.create(
            model=model,
            messages=[
                {"role": "system", "content": COMPONENT1_AMBIGUITY_DETECTION_PROMPT},
                {"role": "user", "content": user_prompt}
            ],
            temperature=0.1,
            max_tokens=1500
        )

        result = parse_llm_json_response(response.choices[0].message.content)

        if result is None:
            return {
                "ambiguity_status": "non-ambiguous",
                "interpretations": [{"id": 1, "interpretation": "Unable to parse - treating as non-ambiguous", "ambiguity_type": None, "missing_information": None}],
                "reasoning": "Failed to parse LLM response",
                "error": True
            }

        return result

    except Exception as e:
        return {
            "ambiguity_status": "non-ambiguous",
            "interpretations": [{"id": 1, "interpretation": "Error occurred - treating as non-ambiguous", "ambiguity_type": None, "missing_information": None}],
            "reasoning": f"Error in ambiguity detection: {str(e)}",
            "error": True
        }


def component2_business_coverage(question, ambiguity_result, business_rules, client, model):
    """
    Component 2: Check if business rules cover detected ambiguities.
    Input: Question, ambiguity result from C1, business rules only
    Output: coverage_status, resolved interpretation or remaining valid interpretations
    """
    try:
        user_prompt = f"""## User Question
{question}

## Detected Ambiguities (from Ambiguity Detection)
{json.dumps(ambiguity_result, indent=2)}

## Business Rules
{business_rules}

---

Analyze whether the business rules resolve the detected ambiguities. Determine the coverage status and identify which interpretation(s) remain valid."""

        response = client.chat.completions.create(
            model=model,
            messages=[
                {"role": "system", "content": COMPONENT2_BUSINESS_COVERAGE_PROMPT},
                {"role": "user", "content": user_prompt}
            ],
            temperature=0.1,
            max_tokens=1500
        )

        result = parse_llm_json_response(response.choices[0].message.content)

        if result is None:
            # Default to not-covered if parsing fails
            return {
                "coverage_status": "not-covered",
                "resolved_interpretation": None,
                "unresolved_ambiguities": [],
                "applied_rules": [],
                "remaining_valid_interpretations": ambiguity_result.get("interpretations", []),
                "reasoning": "Failed to parse LLM response - defaulting to not-covered",
                "error": True
            }

        return result

    except Exception as e:
        return {
            "coverage_status": "not-covered",
            "resolved_interpretation": None,
            "unresolved_ambiguities": [],
            "applied_rules": [],
            "remaining_valid_interpretations": ambiguity_result.get("interpretations", []),
            "reasoning": f"Error in business coverage check: {str(e)}",
            "error": True
        }


def component3_regular_evaluator(question, sql_query, context, expected_interpretation, client, model):
    """
    Component 3: Regular Evaluator - Strict evaluation for non-ambiguous cases.
    Input: Question, SQL, Context, Expected interpretation
    Output: pass/fail, reasoning, issues
    """
    try:
        user_prompt = f"""## Context Information
{context}

---

## Expected Interpretation
{json.dumps(expected_interpretation, indent=2) if isinstance(expected_interpretation, dict) else expected_interpretation}

---

## Question to Evaluate
**User Question:** {question}

**Generated SQL Query:**
```sql
{sql_query}
```

---

Evaluate whether the SQL query correctly implements the expected interpretation. Be strict - there is only ONE correct way to answer this question."""

        response = client.chat.completions.create(
            model=model,
            messages=[
                {"role": "system", "content": COMPONENT3_REGULAR_EVALUATOR_PROMPT},
                {"role": "user", "content": user_prompt}
            ],
            temperature=0.1,
            max_tokens=1500
        )

        result = parse_llm_json_response(response.choices[0].message.content)

        if result is None:
            return {
                "status": "error",
                "confidence": 0.0,
                "reasoning": "Failed to parse LLM response",
                "issues": ["parse_error: Could not parse evaluation response"],
                "expected_interpretation": str(expected_interpretation),
                "actual_implementation": "Unknown"
            }

        return result

    except Exception as e:
        return {
            "status": "error",
            "confidence": 0.0,
            "reasoning": f"Error in evaluation: {str(e)}",
            "issues": [f"evaluation_error: {str(e)}"],
            "expected_interpretation": str(expected_interpretation),
            "actual_implementation": "Unknown"
        }


def component4_ambiguity_evaluator(question, sql_query, context, valid_interpretations, client, model):
    """
    Component 4: Ambiguity Usecase Evaluator - Lenient evaluation for ambiguous cases.
    Input: Question, SQL, Context, List of valid interpretations
    Output: pass/fail (passes if ANY interpretation matches), reasoning, matched interpretation
    """
    try:
        user_prompt = f"""## Context Information
{context}

---

## Valid Interpretations (SQL can match ANY of these)
{json.dumps(valid_interpretations, indent=2)}

---

## Question to Evaluate
**User Question:** {question}

**Generated SQL Query:**
```sql
{sql_query}
```

---

Evaluate whether the SQL query correctly implements ANY ONE of the valid interpretations listed above. The query should PASS if it matches at least one valid interpretation."""

        response = client.chat.completions.create(
            model=model,
            messages=[
                {"role": "system", "content": COMPONENT4_AMBIGUITY_EVALUATOR_PROMPT},
                {"role": "user", "content": user_prompt}
            ],
            temperature=0.1,
            max_tokens=2000
        )

        result = parse_llm_json_response(response.choices[0].message.content)

        if result is None:
            return {
                "status": "error",
                "confidence": 0.0,
                "reasoning": "Failed to parse LLM response",
                "matched_interpretation": None,
                "issues": ["parse_error: Could not parse evaluation response"],
                "interpretation_analysis": []
            }

        return result

    except Exception as e:
        return {
            "status": "error",
            "confidence": 0.0,
            "reasoning": f"Error in evaluation: {str(e)}",
            "matched_interpretation": None,
            "issues": [f"evaluation_error: {str(e)}"],
            "interpretation_analysis": []
        }


def evaluate_query_updated(question, sql_query, context, api_key, model,
                           api_provider="OpenAI", azure_base_url=None, azure_api_version=None):
    """
    Main orchestrator for the updated evaluator logic.
    Routes through Components 1-4 based on ambiguity status and business rule coverage.

    Returns comprehensive evaluation result with all component outputs.
    """
    # Create LLM client
    client = create_llm_client(api_key, api_provider, azure_base_url, azure_api_version)

    # Format context strings
    context_str = format_context_for_prompt(context) if isinstance(context, dict) else str(context)
    business_rules_str = format_business_rules_only(context) if isinstance(context, dict) else "No business rules provided."

    # Initialize result structure
    result = {
        "question": question,
        "sql_query": sql_query,
        "evaluator_type": "updated",

        # Component 1 output
        "ambiguity_analysis": None,

        # Component 2 output (may be null if non-ambiguous)
        "business_coverage": None,

        # Final evaluation
        "evaluation": None,

        # Routing info
        "evaluation_path": None
    }

    # ===========================================
    # STEP 1: Ambiguity Detection (Component 1)
    # ===========================================
    component1_result = component1_ambiguity_detection(question, client, model)
    result["ambiguity_analysis"] = component1_result

    ambiguity_status = component1_result.get("ambiguity_status", "non-ambiguous")
    interpretations = component1_result.get("interpretations", [])

    # ===========================================
    # ROUTING DECISION
    # ===========================================

    if ambiguity_status == "non-ambiguous":
        # NON-AMBIGUOUS PATH: Skip Component 2, go directly to Regular Evaluator
        result["evaluation_path"] = "non-ambiguous -> regular-evaluator"
        result["business_coverage"] = {"status": "not-applicable", "reasoning": "Question is non-ambiguous"}

        # Get the single interpretation
        expected_interpretation = interpretations[0] if interpretations else {"interpretation": "Direct interpretation of question"}

        # Call Component 3: Regular Evaluator
        eval_result = component3_regular_evaluator(
            question, sql_query, context_str, expected_interpretation, client, model
        )

        result["evaluation"] = {
            "evaluator_used": "regular",
            "status": eval_result.get("status", "error"),
            "confidence": eval_result.get("confidence", 0.0),
            "reasoning": eval_result.get("reasoning", ""),
            "issues": eval_result.get("issues", []),
            "expected_interpretation": eval_result.get("expected_interpretation", ""),
            "actual_implementation": eval_result.get("actual_implementation", ""),
            "matched_interpretation": None,
            "interpretation_analysis": None
        }

    else:
        # AMBIGUOUS PATH: Go to Component 2 for business coverage check

        # ===========================================
        # STEP 2: Business Coverage Check (Component 2)
        # ===========================================
        # Pass only business rules to Component 2 (not full context)
        component2_result = component2_business_coverage(
            question, component1_result, business_rules_str, client, model
        )
        result["business_coverage"] = component2_result

        coverage_status = component2_result.get("coverage_status", "not-covered")

        if coverage_status == "fully-covered":
            # FULLY COVERED: Business rules resolve all ambiguities -> Regular Evaluator
            result["evaluation_path"] = "ambiguous -> fully-covered -> regular-evaluator"

            # Get the resolved interpretation
            resolved_interp = component2_result.get("resolved_interpretation", interpretations[0] if interpretations else {})

            # Call Component 3: Regular Evaluator
            eval_result = component3_regular_evaluator(
                question, sql_query, context_str, resolved_interp, client, model
            )

            result["evaluation"] = {
                "evaluator_used": "regular",
                "status": eval_result.get("status", "error"),
                "confidence": eval_result.get("confidence", 0.0),
                "reasoning": eval_result.get("reasoning", ""),
                "issues": eval_result.get("issues", []),
                "expected_interpretation": eval_result.get("expected_interpretation", ""),
                "actual_implementation": eval_result.get("actual_implementation", ""),
                "matched_interpretation": None,
                "interpretation_analysis": None
            }

        else:
            # PARTIALLY/NOT COVERED: Multiple interpretations remain -> Ambiguity Evaluator
            result["evaluation_path"] = f"ambiguous -> {coverage_status} -> ambiguity-evaluator"

            # Get remaining valid interpretations
            valid_interpretations = component2_result.get("remaining_valid_interpretations", interpretations)

            # Call Component 4: Ambiguity Usecase Evaluator
            eval_result = component4_ambiguity_evaluator(
                question, sql_query, context_str, valid_interpretations, client, model
            )

            result["evaluation"] = {
                "evaluator_used": "ambiguity-usecase",
                "status": eval_result.get("status", "error"),
                "confidence": eval_result.get("confidence", 0.0),
                "reasoning": eval_result.get("reasoning", ""),
                "issues": eval_result.get("issues", []),
                "expected_interpretation": None,
                "actual_implementation": None,
                "matched_interpretation": eval_result.get("matched_interpretation"),
                "interpretation_analysis": eval_result.get("interpretation_analysis", [])
            }

    # ===========================================
    # FLATTEN KEY FIELDS FOR COMPATIBILITY
    # ===========================================
    # These fields are used by the existing UI
    result["status"] = result["evaluation"]["status"]
    result["confidence"] = result["evaluation"]["confidence"]
    result["reasoning"] = result["evaluation"]["reasoning"]
    result["issues"] = result["evaluation"]["issues"]

    # Ambiguity fields for backward compatibility
    result["ambiguity_status"] = ambiguity_status
    result["ambiguities_detected"] = [
        {
            "type": interp.get("ambiguity_type", "unknown"),
            "description": interp.get("missing_information", ""),
            "interpretation": interp.get("interpretation", "")
        }
        for interp in interpretations
        if interp.get("ambiguity_type")
    ]

    # Business rule fields
    if result["business_coverage"] and result["business_coverage"].get("status") != "not-applicable":
        result["applied_business_rules"] = result["business_coverage"].get("applied_rules", [])
        if coverage_status == "fully-covered":
            result["business_rule_coverage"] = "business-rule-present"
        elif coverage_status == "not-covered":
            result["business_rule_coverage"] = "business-rule-not-present"
        else:
            result["business_rule_coverage"] = "partially-covered"
    else:
        result["applied_business_rules"] = []
        result["business_rule_coverage"] = "not-applicable"

    return result


def display_results_table(results):
    """Display evaluation results in a formatted table."""
    if not results:
        return

    df = pd.DataFrame(results)

    # Create colored status column
    def color_status(status):
        if status == 'pass':
            return '‚úÖ PASS'
        elif status == 'fail':
            return '‚ùå FAIL'
        elif status == 'error':
            return '‚ö†Ô∏è ERROR'
        else:
            return '‚è≥ PENDING'

    df['Status'] = df['status'].apply(color_status)

    # Display summary metrics
    total = len(results)
    passed = sum(1 for r in results if r['status'] == 'pass')
    failed = sum(1 for r in results if r['status'] == 'fail')
    errors = sum(1 for r in results if r['status'] == 'error')
    pass_rate = (passed / total * 100) if total > 0 else 0

    # Ambiguity metrics
    ambiguous_count = sum(1 for r in results if r.get('ambiguity_status') == 'ambiguous')
    non_ambiguous_count = total - ambiguous_count

    # Summary display - Row 1: Basic metrics
    col1, col2, col3, col4, col5 = st.columns(5)
    with col1:
        st.metric("Total Questions", total)
    with col2:
        st.metric("Passed", passed, delta=None)
    with col3:
        st.metric("Failed", failed, delta=None)
    with col4:
        st.metric("Errors", errors, delta=None)
    with col5:
        st.metric("Pass Rate", f"{pass_rate:.1f}%")

    # Summary display - Row 2: Ambiguity metrics
    col6, col7, col8, col9 = st.columns(4)
    with col6:
        st.metric("Ambiguous Questions", ambiguous_count)
    with col7:
        st.metric("Non-Ambiguous", non_ambiguous_count)
    with col8:
        # Pass rate for ambiguous questions
        ambig_passed = sum(1 for r in results if r.get('ambiguity_status') == 'ambiguous' and r['status'] == 'pass')
        ambig_rate = (ambig_passed / ambiguous_count * 100) if ambiguous_count > 0 else 0
        st.metric("Ambiguous Pass Rate", f"{ambig_rate:.1f}%")
    with col9:
        # Pass rate for non-ambiguous questions
        non_ambig_passed = sum(1 for r in results if r.get('ambiguity_status') != 'ambiguous' and r['status'] == 'pass')
        non_ambig_rate = (non_ambig_passed / non_ambiguous_count * 100) if non_ambiguous_count > 0 else 0
        st.metric("Non-Ambig Pass Rate", f"{non_ambig_rate:.1f}%")

    # Color-coded summary bar
    if pass_rate >= 80:
        summary_color = "#28a745"
    elif pass_rate >= 60:
        summary_color = "#ffc107"
    else:
        summary_color = "#dc3545"

    st.markdown(f"""
    <div class="results-summary" style="background-color: {summary_color}20; border: 2px solid {summary_color};">
        <span style="color: {summary_color};">{passed} of {total} questions passed ({pass_rate:.1f}%)</span>
    </div>
    """, unsafe_allow_html=True)

    # Detailed results table
    st.subheader("Detailed Results")

    for i, result in enumerate(results):
        q_preview = result['question'][:100] + "..." if len(result['question']) > 100 else result['question']
        with st.expander(f"Question {i+1}: {q_preview}"):
            col1, col2 = st.columns([3, 1])
            with col1:
                st.markdown("**Question:**")
                st.write(result['question'])
                st.markdown("**Generated SQL:**")
                st.code(result['sql_query'], language='sql')
            with col2:
                if result['status'] == 'pass':
                    st.success("‚úÖ PASS")
                elif result['status'] == 'fail':
                    st.error("‚ùå FAIL")
                elif result['status'] == 'error':
                    st.warning("‚ö†Ô∏è ERROR")
                else:
                    st.info("‚è≥ PENDING")

                if result.get('confidence'):
                    st.metric("Confidence", f"{result['confidence']*100:.1f}%")

            if result.get('reasoning'):
                st.markdown("**Evaluation Reasoning:**")
                st.info(result['reasoning'])

            if result.get('issues') and len(result['issues']) > 0:
                st.markdown("**Issues Found:**")
                for issue in result['issues']:
                    st.warning(f"‚Ä¢ {issue}")

            # NEW: Evaluation Path and Component Details
            st.markdown("---")

            # Show evaluation path
            eval_path = result.get('evaluation_path', 'unknown')
            evaluator_used = result.get('evaluation', {}).get('evaluator_used', 'unknown') if isinstance(result.get('evaluation'), dict) else 'unknown'

            st.markdown("**Evaluation Path:**")
            if 'ambiguity-evaluator' in eval_path or evaluator_used == 'ambiguity-usecase':
                st.info(f"{eval_path}")
            else:
                st.success(f"{eval_path}")

            col_amb1, col_amb2 = st.columns(2)

            with col_amb1:
                st.markdown("**Ambiguity Status:**")
                amb_status = result.get('ambiguity_status', 'non-ambiguous')
                if amb_status == 'ambiguous':
                    st.warning("üî∏ Ambiguous")
                else:
                    st.success("‚úÖ Non-Ambiguous")

                # Show ambiguities detected (same format as original)
                ambiguities = result.get('ambiguities_detected', [])
                if ambiguities:
                    st.markdown("**Ambiguities Detected:**")
                    st.text(format_ambiguities_for_display(ambiguities))

                # Show interpretations from ambiguity analysis (new detailed view)
                amb_analysis = result.get('ambiguity_analysis', {})
                if isinstance(amb_analysis, dict):
                    interpretations = amb_analysis.get('interpretations', [])
                    if interpretations and len(interpretations) > 1:
                        st.markdown("**Possible Interpretations:**")
                        for interp in interpretations:
                            if isinstance(interp, dict):
                                interp_text = interp.get('interpretation', str(interp))
                                st.caption(f"‚Ä¢ {interp_text[:150]}{'...' if len(interp_text) > 150 else ''}")

            with col_amb2:
                st.markdown("**Business Rule Coverage:**")
                coverage = result.get('business_rule_coverage', 'not-applicable')
                if coverage == 'business-rule-present' or coverage == 'fully-covered':
                    st.success("‚úÖ Fully Covered")
                elif coverage == 'partially-covered':
                    st.warning("üî∏ Partially Covered")
                elif coverage == 'business-rule-not-present' or coverage == 'not-covered':
                    st.error("‚ùå Not Covered")
                else:
                    st.info("‚ûñ Not Applicable")

                rules = result.get('applied_business_rules', [])
                if rules:
                    st.markdown("**Applied Business Rules:**")
                    for rule in rules[:3]:  # Show max 3 rules
                        if isinstance(rule, dict):
                            rule_name = rule.get('rule_name', 'Unknown')
                            st.caption(f"‚Ä¢ {rule_name}")
                        else:
                            st.caption(f"‚Ä¢ {str(rule)[:50]}")

            # Show matched interpretation for ambiguous cases
            evaluation = result.get('evaluation', {})
            if isinstance(evaluation, dict) and evaluation.get('evaluator_used') == 'ambiguity-usecase':
                matched = evaluation.get('matched_interpretation')
                if matched:
                    st.markdown("**Matched Interpretation:**")
                    if isinstance(matched, dict):
                        st.success(f"‚úÖ {matched.get('interpretation', str(matched))[:200]}")
                    else:
                        st.success(f"‚úÖ {str(matched)[:200]}")


# =============================================================================
# MAIN APP LAYOUT
# =============================================================================

st.markdown('<p class="main-header">SQL Query Evaluator</p>', unsafe_allow_html=True)
st.markdown('<p class="sub-header">Validate chatbot-generated SQL queries against your business context</p>', unsafe_allow_html=True)

# Sidebar - Context Upload
with st.sidebar:
    # Quick Start Option
    st.subheader("‚ö° Quick Start")
    use_quick_start = st.checkbox(
        "Use Default Configuration",
        value=st.session_state.use_quick_start,
        help="Load Azure OpenAI configuration and default context files to get started quickly"
    )

    if use_quick_start and not st.session_state.quick_start_loaded:
        with st.spinner("Loading quick start configuration..."):
            loaded_files = load_quick_start_configuration()
            st.success("‚úÖ Quick start configuration loaded!")
            st.session_state.use_quick_start = True
    elif not use_quick_start and st.session_state.quick_start_loaded:
        # Reset to default state if quick start is unchecked
        st.session_state.use_quick_start = False
        st.session_state.quick_start_loaded = False
        st.info("Quick start disabled. You can manually configure settings below.")

    # API Configuration
    st.subheader("üîë API Configuration")

    # API Provider Selection
    default_index = 0 if st.session_state.api_provider == "OpenAI" else 1
    api_provider = st.radio(
        "API Provider",
        ["OpenAI", "Azure OpenAI"],
        index=default_index,
        horizontal=True,
        help="Choose between OpenAI or Azure OpenAI",
        disabled=st.session_state.use_quick_start
    )
    if not st.session_state.use_quick_start:
        st.session_state.api_provider = api_provider

    # API Key Input - separate for each provider
    if api_provider == "Azure OpenAI":
        api_key = st.text_input(
            "Azure OpenAI API Key",
            type="password",
            value="" if st.session_state.use_quick_start else st.session_state.azure_api_key,
            placeholder="Using default config" if st.session_state.use_quick_start else "",
            help="Enter your Azure OpenAI API key" + (" (loaded from secrets)" if st.session_state.use_quick_start else ""),
            disabled=st.session_state.use_quick_start
        )
        if not st.session_state.use_quick_start:
            st.session_state.azure_api_key = api_key

        azure_base_url = st.text_input(
            "Azure Endpoint (Base URL)",
            value="" if st.session_state.use_quick_start else st.session_state.azure_base_url,
            placeholder="Using default config" if st.session_state.use_quick_start else "https://your-resource.openai.azure.com/",
            help="Enter your Azure OpenAI endpoint URL" + (" (loaded from secrets)" if st.session_state.use_quick_start else ""),
            disabled=st.session_state.use_quick_start
        )
        if not st.session_state.use_quick_start:
            st.session_state.azure_base_url = azure_base_url

        azure_api_version = st.text_input(
            "API Version",
            value="" if st.session_state.use_quick_start else st.session_state.azure_api_version,
            placeholder="Using default config" if st.session_state.use_quick_start else "2024-02-01",
            help="Enter the Azure OpenAI API version" + (" (default: 2024-02-01)" if st.session_state.use_quick_start else ""),
            disabled=st.session_state.use_quick_start
        )
        if not st.session_state.use_quick_start:
            st.session_state.azure_api_version = azure_api_version
    else:
        api_key = st.text_input(
            "OpenAI API Key",
            type="password",
            value=st.session_state.openai_api_key,
            help="Enter your OpenAI API key"
        )
        st.session_state.openai_api_key = api_key

    # Model Selection
    if api_provider == "Azure OpenAI":
        model_options = ["gpt-4.1", "gpt-4o"]
        model_help = "Enter your Azure deployment name"
        # Default to gpt-4.1 if quick start is enabled
        default_index = 0 if st.session_state.use_quick_start else 0
    else:
        model_options = ["gpt-4.1", "gpt-4", "gpt-4-turbo", "gpt-4o", "gpt-4o-mini", "gpt-3.5-turbo"]
        model_help = "Select the OpenAI model to use"
        default_index = 0

    selected_model = st.selectbox(
        "Select Model",
        model_options,
        index=default_index,
        help=model_help + (" (default: gpt-4.1)" if st.session_state.use_quick_start else ""),
        disabled=st.session_state.use_quick_start
    )

    # Test API Connection
    if st.button("Test API Connection", use_container_width=True):
        # Use session state values (which hold actual credentials even in quick start mode)
        test_api_key = st.session_state.azure_api_key if api_provider == "Azure OpenAI" else st.session_state.openai_api_key
        test_base_url = st.session_state.azure_base_url
        test_api_version = st.session_state.azure_api_version

        if test_api_key:
            try:
                if api_provider == "Azure OpenAI":
                    if not test_base_url:
                        st.warning("Please enter Azure Base URL")
                    elif not test_api_version:
                        st.warning("Please enter Azure API Version")
                    else:
                        client = AzureOpenAI(
                            api_key=test_api_key,
                            azure_endpoint=test_base_url,
                            api_version=test_api_version
                        )
                        response = client.chat.completions.create(
                            model=selected_model,
                            messages=[{"role": "user", "content": "Say 'API Connected!' in 2 words"}],
                            max_tokens=10
                        )
                        st.success("‚úÖ API Connection Successful!")
                else:
                    client = OpenAI(api_key=test_api_key)
                    response = client.chat.completions.create(
                        model="gpt-3.5-turbo",
                        messages=[{"role": "user", "content": "Say 'API Connected!' in 2 words"}],
                        max_tokens=10
                    )
                    st.success("‚úÖ API Connection Successful!")
            except Exception as e:
                st.error(f"‚ùå API Connection Failed: {str(e)}")
        else:
            st.warning("Please enter an API key first")

    # Show info if quick start is enabled
    if st.session_state.use_quick_start:
        st.info("Quick Start is enabled. Default context files are loaded (shown in each section below). Disable Quick Start to customize files.")

    # Business Rules Upload
    st.subheader("üìã Business Rules")

    # Show quick start loaded files
    if st.session_state.use_quick_start and st.session_state.context_data['business_rules']:
        for item in st.session_state.context_data['business_rules']:
            if isinstance(item, dict) and 'file' in item:
                st.success(f"‚úÖ Loaded: {item['file']}")

    business_rules_files = st.file_uploader(
        "Upload Business Rules",
        type=['yaml', 'yml', 'json', 'txt', 'md'],
        key="business_rules",
        accept_multiple_files=True,
        help="Upload business rules in YAML, JSON, Markdown, or plain text format (multiple files supported)",
        disabled=st.session_state.use_quick_start
    )

    if business_rules_files:
        combined_rules = []
        for file in business_rules_files:
            content = file.read().decode('utf-8')
            if file.name.endswith(('.yaml', '.yml')):
                parsed = parse_yaml_content(content)
                if parsed:
                    combined_rules.append({'file': file.name, 'content': parsed})
            elif file.name.endswith('.json'):
                parsed = parse_json_content(content)
                if parsed:
                    combined_rules.append({'file': file.name, 'content': parsed})
            else:
                # .txt and .md files are treated as plain text
                combined_rules.append({'file': file.name, 'content': content})
            st.success(f"‚úÖ Loaded: {file.name}")
        st.session_state.context_data['business_rules'] = combined_rules

    # KPI Formulas Upload
    st.subheader("üìä KPI Formulas")

    # Show quick start loaded files
    if st.session_state.use_quick_start and st.session_state.context_data['kpi_formulas']:
        for item in st.session_state.context_data['kpi_formulas']:
            if isinstance(item, dict) and 'file' in item:
                st.success(f"‚úÖ Loaded: {item['file']}")

    kpi_formulas_files = st.file_uploader(
        "Upload KPI Formulas",
        type=['yaml', 'yml', 'json', 'txt', 'md'],
        key="kpi_formulas",
        accept_multiple_files=True,
        help="Upload KPI formulas and calculations in YAML, JSON, Markdown, or plain text format (multiple files supported)",
        disabled=st.session_state.use_quick_start
    )

    if kpi_formulas_files:
        combined_formulas = []
        for file in kpi_formulas_files:
            content = file.read().decode('utf-8')
            if file.name.endswith(('.yaml', '.yml')):
                parsed = parse_yaml_content(content)
                if parsed:
                    combined_formulas.append({'file': file.name, 'content': parsed})
            elif file.name.endswith('.json'):
                parsed = parse_json_content(content)
                if parsed:
                    combined_formulas.append({'file': file.name, 'content': parsed})
            else:
                # .txt and .md files are treated as plain text
                combined_formulas.append({'file': file.name, 'content': content})
            st.success(f"‚úÖ Loaded: {file.name}")
        st.session_state.context_data['kpi_formulas'] = combined_formulas

    # Data Dictionary Upload
    st.subheader("üìñ Data Dictionary (DDL)")

    # Show quick start loaded files
    if st.session_state.use_quick_start and st.session_state.context_data['data_dictionary']:
        for item in st.session_state.context_data['data_dictionary']:
            if isinstance(item, dict) and 'file' in item:
                st.success(f"‚úÖ Loaded: {item['file']}")

    data_dict_files = st.file_uploader(
        "Upload Data Dictionary",
        type=['yaml', 'yml', 'json', 'txt', 'sql', 'md'],
        key="data_dictionary",
        accept_multiple_files=True,
        help="Upload DDL statements or data dictionary in YAML, JSON, SQL, Markdown, or plain text format (multiple files supported)",
        disabled=st.session_state.use_quick_start
    )

    if data_dict_files:
        combined_ddl = []
        for file in data_dict_files:
            content = file.read().decode('utf-8')
            if file.name.endswith(('.yaml', '.yml')):
                parsed = parse_yaml_content(content)
                if parsed:
                    combined_ddl.append({'file': file.name, 'content': parsed})
            elif file.name.endswith('.json'):
                parsed = parse_json_content(content)
                if parsed:
                    combined_ddl.append({'file': file.name, 'content': parsed})
            else:
                # .txt, .sql, and .md files are treated as plain text
                combined_ddl.append({'file': file.name, 'content': content})
            st.success(f"‚úÖ Loaded: {file.name}")
        st.session_state.context_data['data_dictionary'] = combined_ddl

    # Table Samples Upload
    st.subheader("üìÑ Table Samples")
    table_samples_files = st.file_uploader(
        "Upload Table Samples (CSV)",
        type=['csv'],
        key="table_samples",
        accept_multiple_files=True,
        help="Upload CSV files with sample data (header + rows)",
        disabled=st.session_state.use_quick_start
    )

    if table_samples_files:
        for file in table_samples_files:
            content = file.read().decode('utf-8')
            table_name = file.name.replace('.csv', '')
            df = parse_csv_content(content)
            if df is not None:
                st.session_state.context_data['table_samples'][table_name] = df
                st.success(f"‚úÖ Loaded: {file.name} ({len(df)} rows)")

    # Context Summary
    st.subheader("üìä Context Summary")
    context_status = []

    rules = st.session_state.context_data['business_rules']
    if rules:
        num_rules = len(rules) if isinstance(rules, list) else 1
        context_status.append(f"‚úÖ Business Rules ({num_rules} file{'s' if num_rules > 1 else ''})")
    else:
        context_status.append("‚¨ú Business Rules")

    formulas = st.session_state.context_data['kpi_formulas']
    if formulas:
        num_formulas = len(formulas) if isinstance(formulas, list) else 1
        context_status.append(f"‚úÖ KPI Formulas ({num_formulas} file{'s' if num_formulas > 1 else ''})")
    else:
        context_status.append("‚¨ú KPI Formulas")

    ddl = st.session_state.context_data['data_dictionary']
    if ddl:
        num_ddl = len(ddl) if isinstance(ddl, list) else 1
        context_status.append(f"‚úÖ Data Dictionary ({num_ddl} file{'s' if num_ddl > 1 else ''})")
    else:
        context_status.append("‚¨ú Data Dictionary")

    num_tables = len(st.session_state.context_data['table_samples'])
    if num_tables > 0:
        context_status.append(f"‚úÖ Table Samples ({num_tables} file{'s' if num_tables > 1 else ''})")
    else:
        context_status.append("‚¨ú Table Samples")

    for status in context_status:
        st.write(status)

    # Clear Context Button
    if st.button("üóëÔ∏è Clear All Context", use_container_width=True):
        st.session_state.context_data = {
            'business_rules': None,
            'kpi_formulas': None,
            'data_dictionary': None,
            'table_samples': {}
        }
        st.rerun()


# Main Content Area
tab1, tab2, tab3, tab4, tab5 = st.tabs(["üìù Input Questions", "üëÅÔ∏è Preview Context", "üìä Results", "üìú History", "‚ÑπÔ∏è About"])

with tab1:
    st.header("Input Questions & SQL Queries")

    # Show quick start loaded questions
    if st.session_state.use_quick_start and st.session_state.questions_data:
        num_questions = len(st.session_state.questions_data)
        st.success(f"‚úÖ Quick Start: Loaded sql-examples_10_test.csv ({num_questions} question-SQL pairs)")

    input_method = st.radio(
        "Choose input method:",
        ["üì§ Upload CSV File", "‚úèÔ∏è Manual Entry", "üìã Paste JSON"],
        horizontal=True
    )

    if input_method == "üì§ Upload CSV File":
        st.markdown("""
        **Expected CSV Format:**
        | question | sql_query |
        |----------|-----------|
        | What is the total sales for Walmart? | SELECT SUM(sales) FROM sales_table WHERE retailer = 'WALMART' |
        """)

        uploaded_file = st.file_uploader(
            "Upload CSV with questions and SQL queries",
            type=['csv'],
            key="questions_csv"
        )

        if uploaded_file:
            df = pd.read_csv(uploaded_file)

            # Check for required columns
            required_cols = ['question', 'sql_query']
            if all(col in df.columns for col in required_cols):
                st.session_state.questions_data = df.to_dict('records')
                st.success(f"‚úÖ Loaded {len(df)} question-SQL pairs")

                # Preview
                st.subheader("Preview")
                st.dataframe(df.head(10), use_container_width=True)
            else:
                st.error(f"CSV must contain columns: {required_cols}")
                st.write(f"Found columns: {list(df.columns)}")

    elif input_method == "‚úèÔ∏è Manual Entry":
        st.markdown("Enter questions and their corresponding SQL queries below:")

        num_entries = st.number_input("Number of entries", min_value=1, max_value=50, value=3)

        manual_entries = []
        for i in range(num_entries):
            st.markdown(f"---\n**Entry {i+1}**")
            col1, col2 = st.columns(2)
            with col1:
                question = st.text_area(f"Question {i+1}", key=f"q_{i}", height=100)
            with col2:
                sql = st.text_area(f"SQL Query {i+1}", key=f"sql_{i}", height=100)

            if question and sql:
                manual_entries.append({'question': question, 'sql_query': sql})

        if st.button("üíæ Save Manual Entries", use_container_width=True):
            if manual_entries:
                st.session_state.questions_data = manual_entries
                st.success(f"‚úÖ Saved {len(manual_entries)} entries")

    elif input_method == "üìã Paste JSON":
        st.markdown("""
        **Expected JSON Format:**
        ```json
        [
            {"question": "What is total sales?", "sql_query": "SELECT SUM(sales) FROM table"},
            {"question": "Show top 5 products", "sql_query": "SELECT * FROM products LIMIT 5"}
        ]
        ```
        """)

        json_input = st.text_area("Paste JSON here:", height=300)

        if st.button("üì• Parse JSON", use_container_width=True):
            if json_input:
                try:
                    data = json.loads(json_input)
                    if isinstance(data, list) and all('question' in item and 'sql_query' in item for item in data):
                        st.session_state.questions_data = data
                        st.success(f"‚úÖ Parsed {len(data)} entries")
                    else:
                        st.error("JSON must be a list of objects with 'question' and 'sql_query' keys")
                except json.JSONDecodeError as e:
                    st.error(f"Invalid JSON: {e}")

    # Current Data Summary
    st.markdown("---")
    st.subheader("üìä Current Data Summary")

    if st.session_state.questions_data:
        st.info(f"**{len(st.session_state.questions_data)}** question-SQL pairs loaded and ready for evaluation")

        # Quick preview
        preview_df = pd.DataFrame(st.session_state.questions_data)
        st.dataframe(preview_df, use_container_width=True, height=200)
    else:
        st.warning("No questions loaded. Please upload or enter question-SQL pairs.")

    # Evaluation Button
    st.markdown("---")
    col1, col2, col3 = st.columns([1, 2, 1])
    with col2:
        # Check if API key is present based on provider
        current_api_key = st.session_state.azure_api_key if st.session_state.api_provider == "Azure OpenAI" else st.session_state.openai_api_key

        evaluate_button = st.button(
            "üöÄ Run Evaluation",
            use_container_width=True,
            type="primary",
            disabled=not st.session_state.questions_data or not current_api_key
        )

        if not current_api_key:
            st.caption("‚ö†Ô∏è Please enter your API key in the sidebar")
        if not st.session_state.questions_data:
            st.caption("‚ö†Ô∏è Please load questions first")

    if evaluate_button:
        # Format context for evaluation
        formatted_context = format_context_for_prompt(st.session_state.context_data)

        if not formatted_context:
            st.warning("‚ö†Ô∏è No context loaded. Evaluation will proceed without business context.")
            formatted_context = "No context provided. Evaluate based on general SQL best practices."

        with st.spinner("Evaluating SQL queries using OpenAI..."):
            results = []
            progress_bar = st.progress(0)
            status_text = st.empty()

            total = len(st.session_state.questions_data)
            for i, item in enumerate(st.session_state.questions_data):
                status_text.text(f"Evaluating question {i+1} of {total}...")
                progress_bar.progress((i + 1) / total)

                # Get the appropriate API key based on provider
                current_api_key = st.session_state.azure_api_key if st.session_state.api_provider == "Azure OpenAI" else st.session_state.openai_api_key

                # Call Updated Evaluator (multi-component architecture)
                result = evaluate_query_updated(
                    question=item['question'],
                    sql_query=item['sql_query'],
                    context=st.session_state.context_data,
                    api_key=current_api_key,
                    model=selected_model,
                    api_provider=st.session_state.api_provider,
                    azure_base_url=st.session_state.get('azure_base_url'),
                    azure_api_version=st.session_state.get('azure_api_version')
                )
                results.append(result)

                # Small delay to avoid rate limiting
                time.sleep(0.5)

            st.session_state.evaluation_results = results

            # Save to history
            history_entry = {
                'timestamp': datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                'total': total,
                'passed': sum(1 for r in results if r['status'] == 'pass'),
                'failed': sum(1 for r in results if r['status'] == 'fail'),
                'errors': sum(1 for r in results if r['status'] == 'error'),
                'model': selected_model,
                'results': results
            }
            st.session_state.evaluation_history.append(history_entry)

            progress_bar.empty()
            status_text.text("‚úÖ Evaluation complete!")
            st.success("Evaluation complete! View results in the Results tab.")


with tab2:
    st.header("Preview Loaded Context")

    # Business Rules Preview
    with st.expander("üìã Business Rules", expanded=True):
        rules = st.session_state.context_data['business_rules']
        if rules:
            if isinstance(rules, list):
                # Multiple files format
                for item in rules:
                    if isinstance(item, dict) and 'file' in item:
                        st.markdown(f"**üìÑ {item['file']}**")
                        content = item.get('content', '')
                        if isinstance(content, dict):
                            st.json(content)
                        elif item['file'].endswith('.md'):
                            st.markdown(content)
                        else:
                            st.text(str(content))
                        st.markdown("---")
            elif isinstance(rules, dict):
                st.json(rules)
            else:
                st.text(str(rules))
        else:
            st.info("No business rules loaded")

    # KPI Formulas Preview
    with st.expander("üìä KPI Formulas", expanded=True):
        formulas = st.session_state.context_data['kpi_formulas']
        if formulas:
            if isinstance(formulas, list):
                # Multiple files format
                for item in formulas:
                    if isinstance(item, dict) and 'file' in item:
                        st.markdown(f"**üìÑ {item['file']}**")
                        content = item.get('content', '')
                        if isinstance(content, dict):
                            st.json(content)
                        elif item['file'].endswith('.md'):
                            st.markdown(content)
                        else:
                            st.text(str(content))
                        st.markdown("---")
            elif isinstance(formulas, dict):
                st.json(formulas)
            else:
                st.text(str(formulas))
        else:
            st.info("No KPI formulas loaded")

    # Data Dictionary Preview
    with st.expander("üìñ Data Dictionary (DDL)", expanded=True):
        ddl = st.session_state.context_data['data_dictionary']
        if ddl:
            if isinstance(ddl, list):
                # Multiple files format
                for item in ddl:
                    if isinstance(item, dict) and 'file' in item:
                        st.markdown(f"**üìÑ {item['file']}**")
                        content = item.get('content', '')
                        if isinstance(content, dict):
                            st.json(content)
                        elif isinstance(content, list):
                            for sub_item in content:
                                if isinstance(sub_item, dict) and 'ddl' in sub_item:
                                    st.code(sub_item['ddl'], language='sql')
                                else:
                                    st.json(sub_item)
                        elif item['file'].endswith('.md'):
                            st.markdown(content)
                        elif item['file'].endswith('.sql'):
                            st.code(str(content), language='sql')
                        else:
                            st.text(str(content))
                        st.markdown("---")
                    elif isinstance(item, dict) and 'ddl' in item:
                        st.code(item['ddl'], language='sql')
                    else:
                        st.json(item)
            elif isinstance(ddl, dict):
                st.json(ddl)
            else:
                st.code(str(ddl), language='sql')
        else:
            st.info("No data dictionary loaded")

    # Table Samples Preview
    with st.expander("üìÑ Table Samples", expanded=True):
        if st.session_state.context_data['table_samples']:
            for table_name, df in st.session_state.context_data['table_samples'].items():
                st.markdown(f"**{table_name}** ({len(df)} rows)")
                st.dataframe(df, use_container_width=True, height=200)
        else:
            st.info("No table samples loaded")

    # Full Context Preview
    st.markdown("---")
    st.subheader("üìù Full Context (as sent to LLM)")
    formatted_context = format_context_for_prompt(st.session_state.context_data)
    if formatted_context:
        st.text_area("Context Preview", formatted_context, height=400)
        st.caption(f"Total context length: {len(formatted_context)} characters (~{len(formatted_context)//4} tokens)")
    else:
        st.info("No context loaded yet")


with tab3:
    st.header("Evaluation Results")

    if st.session_state.evaluation_results:
        # Export options
        col1, col2, col3, col4 = st.columns([1, 1, 1, 1])
        with col1:
            # Export as PDF Report
            if st.button("üìÑ Generate PDF Report", use_container_width=True):
                try:
                    with st.spinner("Generating PDF report..."):
                        # Get API credentials from session state
                        api_key = st.session_state.azure_api_key if st.session_state.api_provider == "Azure OpenAI" else st.session_state.openai_api_key

                        if not api_key:
                            st.error("API key required for report generation")
                        else:
                            # Generate PDF
                            pdf_bytes = generate_evaluation_pdf_report(
                                st.session_state.evaluation_results,
                                api_key,
                                selected_model,
                                st.session_state.api_provider,
                                st.session_state.get('azure_base_url'),
                                st.session_state.get('azure_api_version')
                            )

                            # Show download button
                            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                            st.download_button(
                                "‚¨áÔ∏è Download PDF",
                                pdf_bytes,
                                f"sql_evaluation_report_{timestamp}.pdf",
                                "application/pdf",
                                use_container_width=True
                            )
                            st.success("PDF report generated successfully!")
                except ValueError as e:
                    st.error(f"Cannot generate report: {str(e)}")
                except Exception as e:
                    st.error(f"Failed to generate PDF: {str(e)}")
        with col2:
            # Export as Excel - Flatten nested structures for Excel
            export_data = []
            for result in st.session_state.evaluation_results:
                # Extract interpretations from ambiguity_analysis
                amb_analysis = result.get('ambiguity_analysis', {})
                interpretations = []
                if isinstance(amb_analysis, dict):
                    interp_list = amb_analysis.get('interpretations', [])
                    for interp in interp_list:
                        if isinstance(interp, dict):
                            interpretations.append(interp.get('interpretation', str(interp)))
                        else:
                            interpretations.append(str(interp))

                # Extract matched interpretation from evaluation
                evaluation = result.get('evaluation', {})
                matched_interp = ''
                if isinstance(evaluation, dict):
                    matched = evaluation.get('matched_interpretation')
                    if matched:
                        if isinstance(matched, dict):
                            matched_interp = matched.get('interpretation', str(matched))
                        else:
                            matched_interp = str(matched)

                # Extract expected interpretation
                expected_interp = result.get('expected_interpretation', '')
                if isinstance(expected_interp, dict):
                    expected_interp = expected_interp.get('interpretation', str(expected_interp))

                flat_result = {
                    'question': result.get('question'),
                    'sql_query': result.get('sql_query'),
                    'status': result.get('status'),
                    'reasoning': result.get('reasoning'),
                    'issues': '; '.join(result.get('issues', [])),
                    'confidence': result.get('confidence'),

                    # Evaluation path
                    'evaluation_path': result.get('evaluation_path', ''),

                    # Ambiguity fields
                    'ambiguity_status': result.get('ambiguity_status'),
                    'num_ambiguities': len(result.get('ambiguities_detected', [])),
                    'ambiguity_types': '; '.join([
                        amb.get('type', '') for amb in result.get('ambiguities_detected', [])
                        if isinstance(amb, dict)
                    ]),
                    'ambiguity_descriptions': ' | '.join([
                        amb.get('description', '') for amb in result.get('ambiguities_detected', [])
                        if isinstance(amb, dict)
                    ]),

                    # Interpretations
                    'possible_interpretations': ' | '.join(interpretations),
                    'expected_interpretation': expected_interp,
                    'matched_interpretation': matched_interp,

                    # Business rule fields
                    'business_rule_coverage': result.get('business_rule_coverage'),
                    'num_rules_applied': len(result.get('applied_business_rules', [])),
                    'applied_rule_names': '; '.join([
                        rule.get('rule_name', '') for rule in result.get('applied_business_rules', [])
                        if isinstance(rule, dict)
                    ]),
                    'applied_rule_assumptions': ' | '.join([
                        f"{rule.get('rule_name', '')}: {rule.get('assumption', '')}"
                        for rule in result.get('applied_business_rules', [])
                        if isinstance(rule, dict)
                    ])
                }
                export_data.append(flat_result)

            export_df = pd.DataFrame(export_data)
            excel_buffer = BytesIO()
            export_df.to_excel(excel_buffer, index=False, engine='openpyxl')
            excel_buffer.seek(0)
            st.download_button(
                "üì• Export Excel",
                excel_buffer,
                "evaluation_results.xlsx",
                "application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
                use_container_width=True
            )
        with col3:
            # Export as JSON
            json_str = json.dumps(st.session_state.evaluation_results, indent=2)
            st.download_button(
                "üì• Export JSON",
                json_str,
                "evaluation_results.json",
                "application/json",
                use_container_width=True
            )

        # Display overall summary
        results = st.session_state.evaluation_results
        total = len(results)
        passed = sum(1 for r in results if r['status'] == 'pass')
        failed = sum(1 for r in results if r['status'] == 'fail')
        errors = sum(1 for r in results if r['status'] == 'error')
        pass_rate = (passed / total * 100) if total > 0 else 0

        # Summary metrics - Row 1
        col1, col2, col3, col4, col5 = st.columns(5)
        with col1:
            st.metric("Total Questions", total)
        with col2:
            st.metric("Passed", passed, delta=None)
        with col3:
            st.metric("Failed", failed, delta=None)
        with col4:
            st.metric("Errors", errors, delta=None)
        with col5:
            st.metric("Pass Rate", f"{pass_rate:.1f}%")

        # Summary metrics - Row 2: Ambiguity metrics
        ambiguous_count = sum(1 for r in results if r.get('ambiguity_status') == 'ambiguous')
        non_ambiguous_count = total - ambiguous_count
        ambig_passed = sum(1 for r in results if r.get('ambiguity_status') == 'ambiguous' and r['status'] == 'pass')
        non_ambig_passed = sum(1 for r in results if r.get('ambiguity_status') != 'ambiguous' and r['status'] == 'pass')
        ambig_rate = (ambig_passed / ambiguous_count * 100) if ambiguous_count > 0 else 0
        non_ambig_rate = (non_ambig_passed / non_ambiguous_count * 100) if non_ambiguous_count > 0 else 0

        col6, col7, col8, col9 = st.columns(4)
        with col6:
            st.metric("Ambiguous Questions", ambiguous_count)
        with col7:
            st.metric("Non-Ambiguous", non_ambiguous_count)
        with col8:
            st.metric("Ambiguous Pass Rate", f"{ambig_rate:.1f}%")
        with col9:
            st.metric("Non-Ambig Pass Rate", f"{non_ambig_rate:.1f}%")

        # Color-coded summary bar
        if pass_rate >= 80:
            summary_color = "#28a745"
        elif pass_rate >= 60:
            summary_color = "#ffc107"
        else:
            summary_color = "#dc3545"

        st.markdown(f"""
        <div class="results-summary" style="background-color: {summary_color}20; border: 2px solid {summary_color};">
            <span style="color: {summary_color};">{passed} of {total} questions passed ({pass_rate:.1f}%)</span>
        </div>
        """, unsafe_allow_html=True)

        st.markdown("---")

        # Performance Overview Table
        st.subheader("üìä Performance Overview")

        # Preprocess data for detailed metrics
        preprocessed = preprocess_evaluation_results(results)

        performance_data = {
            "Metric": [
                "Total Queries Evaluated",
                "Passed Queries",
                "Failed Queries",
                "Error Queries",
                "Pass Rate",
                "Average Confidence Score",
                "Queries with Issues",
                "Total Issues Identified",
                "Average Issues per Failed Query"
            ],
            "Value": [
                preprocessed['total_queries'],
                f"{preprocessed['passed_count']} ({preprocessed['passed_count']/preprocessed['total_queries']*100:.1f}%)",
                f"{preprocessed['failed_count']} ({preprocessed['failed_count']/preprocessed['total_queries']*100:.1f}%)",
                f"{preprocessed['error_count']} ({preprocessed['error_count']/preprocessed['total_queries']*100:.1f}%)",
                f"{preprocessed['pass_rate']:.1f}%",
                f"{preprocessed['avg_confidence']:.2f} / 1.00",
                preprocessed['failed_queries_count'],
                preprocessed['total_issues'],
                f"{preprocessed['avg_issues_per_failed']:.1f}"
            ]
        }

        performance_df = pd.DataFrame(performance_data)
        st.dataframe(performance_df, use_container_width=True, hide_index=True)

        st.markdown("---")

        # Issue Distribution Table
        st.subheader("üîç Issue Distribution")

        if preprocessed['top_10_issues']:
            st.markdown("The following table shows the most frequently occurring issues and the percentage of queries affected by each issue:")

            issues_data = {
                "Issue Type": [issue['issue'] for issue in preprocessed['top_10_issues']],
                "Occurrences": [issue['count'] for issue in preprocessed['top_10_issues']],
                "% of Queries": [f"{issue['percentage']:.1f}%" for issue in preprocessed['top_10_issues']]
            }

            issues_df = pd.DataFrame(issues_data)
            st.dataframe(issues_df, use_container_width=True, hide_index=True)
        else:
            st.success("No issues found - all queries passed successfully!")

        st.markdown("---")

        # NEW: Ambiguity Analysis Section
        st.subheader("üîç Ambiguity Analysis")

        if preprocessed['ambiguous_count'] > 0:
            col1, col2, col3, col4 = st.columns(4)

            with col1:
                st.metric("Ambiguous Queries", preprocessed['ambiguous_count'],
                          f"{preprocessed['ambiguous_rate']:.1f}%")

            with col2:
                st.metric("Rule Coverage", preprocessed['rule_present_count'],
                          f"{preprocessed['rule_coverage_rate']:.1f}%")

            with col3:
                st.metric("Pass (w/ Rules)", preprocessed['pass_ambiguous_rule_present'])

            with col4:
                st.metric("Fail (No Rules)", preprocessed['fail_ambiguous_no_rule'])

            # Status breakdown table - All combinations
            st.markdown("**Status Combinations:**")
            status_breakdown = []
            total = preprocessed['total_queries']

            # Non-Ambiguous combinations
            if preprocessed['pass_non_ambiguous'] > 0:
                status_breakdown.append(("Pass | Non-Ambiguous", preprocessed['pass_non_ambiguous']))
            if preprocessed['fail_non_ambiguous'] > 0:
                status_breakdown.append(("Fail | Non-Ambiguous", preprocessed['fail_non_ambiguous']))
            if preprocessed.get('error_non_ambiguous', 0) > 0:
                status_breakdown.append(("Error | Non-Ambiguous", preprocessed['error_non_ambiguous']))

            # Ambiguous + Fully Covered combinations
            if preprocessed.get('pass_ambiguous_fully_covered', 0) > 0:
                status_breakdown.append(("Pass | Ambiguous | Fully Covered", preprocessed['pass_ambiguous_fully_covered']))
            if preprocessed.get('fail_ambiguous_fully_covered', 0) > 0:
                status_breakdown.append(("Fail | Ambiguous | Fully Covered", preprocessed['fail_ambiguous_fully_covered']))
            if preprocessed.get('error_ambiguous_fully_covered', 0) > 0:
                status_breakdown.append(("Error | Ambiguous | Fully Covered", preprocessed['error_ambiguous_fully_covered']))

            # Ambiguous + Partially Covered combinations
            if preprocessed.get('pass_ambiguous_partial', 0) > 0:
                status_breakdown.append(("Pass | Ambiguous | Partially Covered", preprocessed['pass_ambiguous_partial']))
            if preprocessed.get('fail_ambiguous_partial', 0) > 0:
                status_breakdown.append(("Fail | Ambiguous | Partially Covered", preprocessed['fail_ambiguous_partial']))
            if preprocessed.get('error_ambiguous_partial', 0) > 0:
                status_breakdown.append(("Error | Ambiguous | Partially Covered", preprocessed['error_ambiguous_partial']))

            # Ambiguous + Not Covered combinations
            if preprocessed.get('pass_ambiguous_no_rule', 0) > 0:
                status_breakdown.append(("Pass | Ambiguous | No Rule", preprocessed['pass_ambiguous_no_rule']))
            if preprocessed.get('fail_ambiguous_no_rule', 0) > 0:
                status_breakdown.append(("Fail | Ambiguous | No Rule", preprocessed['fail_ambiguous_no_rule']))
            if preprocessed.get('error_ambiguous_no_rule', 0) > 0:
                status_breakdown.append(("Error | Ambiguous | No Rule", preprocessed['error_ambiguous_no_rule']))

            # Create dataframe with only non-zero combinations, sorted by count descending
            if status_breakdown:
                # Sort by count descending
                status_breakdown.sort(key=lambda x: x[1], reverse=True)
                breakdown_df = pd.DataFrame([
                    {"Status Combination": combo, "Count": count, "Percentage": f"{(count/total*100):.1f}%"}
                    for combo, count in status_breakdown
                ])
                st.dataframe(breakdown_df, use_container_width=True, hide_index=True)
            else:
                st.info("No status combinations found")

            st.markdown("---")

            # Most common ambiguities
            st.subheader("üìä Most Common Ambiguities")
            if preprocessed['top_10_ambiguity_types']:
                amb_types_df = pd.DataFrame({
                    "Ambiguity Type": [item['type'] for item in preprocessed['top_10_ambiguity_types']],
                    "Occurrences": [item['count'] for item in preprocessed['top_10_ambiguity_types']],
                    "% of Ambiguous": [f"{item['percentage']:.1f}%" for item in preprocessed['top_10_ambiguity_types']]
                })
                st.dataframe(amb_types_df, use_container_width=True, hide_index=True)
            else:
                st.info("No ambiguity types detected")

            st.markdown("---")

            # Most applied rules
            st.subheader("üìã Most Applied Business Rules")
            if preprocessed['top_10_applied_rules']:
                rules_df = pd.DataFrame({
                    "Business Rule": [item['rule'] for item in preprocessed['top_10_applied_rules']],
                    "Times Applied": [item['count'] for item in preprocessed['top_10_applied_rules']],
                    "% of Queries": [f"{item['percentage']:.1f}%" for item in preprocessed['top_10_applied_rules']]
                })
                st.dataframe(rules_df, use_container_width=True, hide_index=True)
            else:
                st.info("No business rules were applied")

            st.markdown("---")
        else:
            st.success("‚úÖ No ambiguous queries detected - all questions were clear and complete!")

        st.markdown("---")

        # NEW: Filtering Controls
        st.subheader("üîß Filter Results")

        col_filter1, col_filter2, col_filter3 = st.columns(3)

        with col_filter1:
            filter_status = st.selectbox("Filter by Status",
                                          ["All", "Pass", "Fail", "Error"],
                                          key="filter_status")

        with col_filter2:
            filter_ambiguity = st.selectbox("Filter by Ambiguity",
                                             ["All", "Ambiguous", "Non-Ambiguous"],
                                             key="filter_ambiguity")

        with col_filter3:
            filter_rule_coverage = st.selectbox("Filter by Rule Coverage",
                                                 ["All", "Fully Covered", "Partially Covered", "No Rule", "Not Applicable"],
                                                 key="filter_rule_coverage")

        # Apply filters
        filtered_results = results.copy()

        if filter_status != "All":
            filtered_results = [r for r in filtered_results if r['status'] == filter_status.lower()]

        if filter_ambiguity != "All":
            amb_map = {"Ambiguous": "ambiguous", "Non-Ambiguous": "non-ambiguous"}
            filtered_results = [r for r in filtered_results if r.get('ambiguity_status') == amb_map[filter_ambiguity]]

        if filter_rule_coverage != "All":
            cov_map = {"Fully Covered": "business-rule-present",
                       "Partially Covered": "partially-covered",
                       "No Rule": "business-rule-not-present",
                       "Not Applicable": "not-applicable"}
            filtered_results = [r for r in filtered_results if r.get('business_rule_coverage') == cov_map[filter_rule_coverage]]

        st.info(f"Showing {len(filtered_results)} of {len(results)} queries")
        st.markdown("---")

        # Separate sections for PASS and FAIL (using filtered_results now)
        passed_results = [r for r in filtered_results if r['status'] == 'pass']
        failed_results = [r for r in filtered_results if r['status'] == 'fail']
        error_results = [r for r in filtered_results if r['status'] == 'error']

        # PASSED Section
        st.markdown("## Passed Queries")
        if passed_results:
            st.success(f"**{len(passed_results)} queries passed evaluation**")
            for i, result in enumerate(passed_results):
                q_preview = result['question'][:100] + "..." if len(result['question']) > 100 else result['question']
                with st.expander(f"Question {i+1}: {q_preview}"):
                    col1, col2 = st.columns([3, 1])
                    with col1:
                        st.markdown("**Question:**")
                        st.write(result['question'])
                        st.markdown("**Generated SQL:**")
                        st.code(result['sql_query'], language='sql')
                    with col2:
                        st.success("‚úÖ PASS")
                        if result.get('confidence'):
                            st.metric("Confidence", f"{result['confidence']*100:.1f}%")

                    if result.get('reasoning'):
                        st.markdown("**Evaluation Reasoning:**")
                        st.info(result['reasoning'])

                    # Evaluation Path and Ambiguity information for passed queries
                    st.markdown("---")

                    # Show evaluation path
                    eval_path = result.get('evaluation_path', 'unknown')
                    st.markdown("**Evaluation Path:**")
                    if 'ambiguity-evaluator' in str(eval_path):
                        st.info(f"{eval_path}")
                    else:
                        st.success(f"{eval_path}")

                    col_amb1, col_amb2 = st.columns(2)

                    with col_amb1:
                        st.markdown("**Ambiguity Status:**")
                        amb_status = result.get('ambiguity_status', 'non-ambiguous')
                        if amb_status == 'ambiguous':
                            st.warning("üî∏ Ambiguous")
                        else:
                            st.success("‚úÖ Non-Ambiguous")

                        ambiguities = result.get('ambiguities_detected', [])
                        if ambiguities:
                            st.markdown("**Ambiguities Detected:**")
                            st.text(format_ambiguities_for_display(ambiguities))

                        # Show interpretations for ambiguous questions
                        amb_analysis = result.get('ambiguity_analysis', {})
                        if isinstance(amb_analysis, dict) and amb_status == 'ambiguous':
                            interpretations = amb_analysis.get('interpretations', [])
                            if interpretations and len(interpretations) > 1:
                                st.markdown("**Possible Interpretations:**")
                                for interp in interpretations:
                                    if isinstance(interp, dict):
                                        interp_text = interp.get('interpretation', str(interp))
                                        st.caption(f"‚Ä¢ {interp_text[:150]}{'...' if len(str(interp_text)) > 150 else ''}")

                    with col_amb2:
                        st.markdown("**Business Rule Coverage:**")
                        coverage = result.get('business_rule_coverage', 'not-applicable')
                        if coverage == 'business-rule-present' or coverage == 'fully-covered':
                            st.success("‚úÖ Fully Covered")
                        elif coverage == 'partially-covered':
                            st.warning("üî∏ Partially Covered")
                        elif coverage == 'business-rule-not-present' or coverage == 'not-covered':
                            st.error("‚ùå Not Covered")
                        else:
                            st.info("‚ûñ Not Applicable")

                        rules = result.get('applied_business_rules', [])
                        if rules:
                            st.markdown("**Applied Business Rules:**")
                            st.text(format_rules_for_display(rules))

                        # Show matched interpretation for ambiguous cases
                        evaluation = result.get('evaluation', {})
                        if isinstance(evaluation, dict) and evaluation.get('evaluator_used') == 'ambiguity-usecase':
                            matched = evaluation.get('matched_interpretation')
                            if matched:
                                st.markdown("**Matched Interpretation:**")
                                if isinstance(matched, dict):
                                    st.success(f"‚úÖ {matched.get('interpretation', str(matched))[:150]}")
                                else:
                                    st.success(f"‚úÖ {str(matched)[:150]}")
        else:
            st.info("No queries passed evaluation")

        st.markdown("---")

        # FAILED Section
        st.markdown("## Failed Queries")
        if failed_results:
            st.error(f"**{len(failed_results)} queries failed evaluation**")
            for i, result in enumerate(failed_results):
                q_preview = result['question'][:100] + "..." if len(result['question']) > 100 else result['question']
                with st.expander(f"Question {i+1}: {q_preview}"):
                    col1, col2 = st.columns([3, 1])
                    with col1:
                        st.markdown("**Question:**")
                        st.write(result['question'])
                        st.markdown("**Generated SQL:**")
                        st.code(result['sql_query'], language='sql')
                    with col2:
                        st.error("‚ùå FAIL")
                        if result.get('confidence'):
                            st.metric("Confidence", f"{result['confidence']*100:.1f}%")

                    if result.get('reasoning'):
                        st.markdown("**Evaluation Reasoning:**")
                        st.info(result['reasoning'])

                    if result.get('issues') and len(result['issues']) > 0:
                        st.markdown("**Issues Found:**")
                        for issue in result['issues']:
                            st.warning(f"‚Ä¢ {issue}")

                    # Evaluation Path and Ambiguity information for failed queries
                    st.markdown("---")

                    # Show evaluation path
                    eval_path = result.get('evaluation_path', 'unknown')
                    st.markdown("**Evaluation Path:**")
                    if 'ambiguity-evaluator' in str(eval_path):
                        st.info(f"{eval_path}")
                    else:
                        st.success(f"{eval_path}")

                    col_amb1, col_amb2 = st.columns(2)

                    with col_amb1:
                        st.markdown("**Ambiguity Status:**")
                        amb_status = result.get('ambiguity_status', 'non-ambiguous')
                        if amb_status == 'ambiguous':
                            st.warning("üî∏ Ambiguous")
                        else:
                            st.success("‚úÖ Non-Ambiguous")

                        ambiguities = result.get('ambiguities_detected', [])
                        if ambiguities:
                            st.markdown("**Ambiguities Detected:**")
                            st.text(format_ambiguities_for_display(ambiguities))

                        # Show interpretations for ambiguous questions
                        amb_analysis = result.get('ambiguity_analysis', {})
                        if isinstance(amb_analysis, dict) and amb_status == 'ambiguous':
                            interpretations = amb_analysis.get('interpretations', [])
                            if interpretations and len(interpretations) > 1:
                                st.markdown("**Possible Interpretations:**")
                                for interp in interpretations:
                                    if isinstance(interp, dict):
                                        interp_text = interp.get('interpretation', str(interp))
                                        st.caption(f"‚Ä¢ {interp_text[:150]}{'...' if len(str(interp_text)) > 150 else ''}")

                    with col_amb2:
                        st.markdown("**Business Rule Coverage:**")
                        coverage = result.get('business_rule_coverage', 'not-applicable')
                        if coverage == 'business-rule-present' or coverage == 'fully-covered':
                            st.success("‚úÖ Fully Covered")
                        elif coverage == 'partially-covered':
                            st.warning("üî∏ Partially Covered")
                        elif coverage == 'business-rule-not-present' or coverage == 'not-covered':
                            st.error("‚ùå Not Covered")
                        else:
                            st.info("‚ûñ Not Applicable")

                        rules = result.get('applied_business_rules', [])
                        if rules:
                            st.markdown("**Applied Business Rules:**")
                            st.text(format_rules_for_display(rules))
        else:
            st.info("No queries failed evaluation")

        # ERRORS Section (if any)
        if error_results:
            st.markdown("---")
            st.markdown("## Errors")
            st.warning(f"**{len(error_results)} queries had errors during evaluation**")
            for i, result in enumerate(error_results):
                q_preview = result['question'][:100] + "..." if len(result['question']) > 100 else result['question']
                with st.expander(f"Question {i+1}: {q_preview}"):
                    col1, col2 = st.columns([3, 1])
                    with col1:
                        st.markdown("**Question:**")
                        st.write(result['question'])
                        st.markdown("**Generated SQL:**")
                        st.code(result['sql_query'], language='sql')
                    with col2:
                        st.warning("‚ö†Ô∏è ERROR")

                    if result.get('reasoning'):
                        st.markdown("**Error Details:**")
                        st.error(result['reasoning'])

                    if result.get('issues') and len(result['issues']) > 0:
                        st.markdown("**Issues:**")
                        for issue in result['issues']:
                            st.warning(f"‚Ä¢ {issue}")

                    # Evaluation Path and Ambiguity information for error queries
                    st.markdown("---")

                    # Show evaluation path
                    eval_path = result.get('evaluation_path', 'unknown')
                    st.markdown("**Evaluation Path:**")
                    st.warning(f"{eval_path}")

                    col_amb1, col_amb2 = st.columns(2)

                    with col_amb1:
                        st.markdown("**Ambiguity Status:**")
                        amb_status = result.get('ambiguity_status', 'non-ambiguous')
                        if amb_status == 'ambiguous':
                            st.warning("üî∏ Ambiguous")
                        else:
                            st.success("‚úÖ Non-Ambiguous")

                        ambiguities = result.get('ambiguities_detected', [])
                        if ambiguities:
                            st.markdown("**Ambiguities Detected:**")
                            st.text(format_ambiguities_for_display(ambiguities))

                    with col_amb2:
                        st.markdown("**Business Rule Coverage:**")
                        coverage = result.get('business_rule_coverage', 'not-applicable')
                        if coverage == 'business-rule-present' or coverage == 'fully-covered':
                            st.success("‚úÖ Fully Covered")
                        elif coverage == 'partially-covered':
                            st.warning("üî∏ Partially Covered")
                        elif coverage == 'business-rule-not-present' or coverage == 'not-covered':
                            st.error("‚ùå Not Covered")
                        else:
                            st.info("‚ûñ Not Applicable")

                        rules = result.get('applied_business_rules', [])
                        if rules:
                            st.markdown("**Applied Business Rules:**")
                            st.text(format_rules_for_display(rules))
    else:
        st.info("No evaluation results yet. Run an evaluation from the Input Questions tab.")

        # Show sample results for demo
        if st.checkbox("Show sample results (demo)"):
            sample_results = [
                {
                    'question': 'What is the total sales for Walmart in 2024?',
                    'sql_query': "SELECT SUM(sales_value) FROM fact_sales WHERE retailer = 'WALMART' AND YEAR(date) = 2024",
                    'status': 'pass',
                    'reasoning': 'Query correctly filters by retailer and year, uses appropriate aggregation.',
                    'issues': [],
                    'confidence': 0.95
                },
                {
                    'question': 'Show OOS percentage for Lysol brand',
                    'sql_query': "SELECT AVG(oos_percentage) FROM inventory WHERE brand = 'Lysol'",
                    'status': 'fail',
                    'reasoning': 'Query uses incorrect OOS calculation formula. Should use the formula: SUM(agg_complete_oos) / SUM(agg_sales)',
                    'issues': ['Incorrect OOS formula', 'Wrong table used'],
                    'confidence': 0.88
                },
                {
                    'question': 'Top 5 products by display compliance',
                    'sql_query': "SELECT product_name, SUM(agg_total_compliance_hybrid) / NULLIF(SUM(agg_sales_dp_hybrid), 0) as compliance FROM fact_display GROUP BY product_name ORDER BY compliance DESC LIMIT 5",
                    'status': 'pass',
                    'reasoning': 'Correctly uses the display compliance formula with proper NULL handling.',
                    'issues': [],
                    'confidence': 0.92
                }
            ]
            display_results_table(sample_results)


with tab4:
    st.header("Evaluation History")

    if st.session_state.evaluation_history:
        for i, entry in enumerate(reversed(st.session_state.evaluation_history)):
            pass_rate = (entry['passed'] / entry['total'] * 100) if entry['total'] > 0 else 0

            with st.expander(f"üìÖ {entry['timestamp']} | {entry['passed']}/{entry['total']} passed ({pass_rate:.1f}%) | Model: {entry['model']}"):
                col1, col2, col3, col4 = st.columns(4)
                with col1:
                    st.metric("Total", entry['total'])
                with col2:
                    st.metric("Passed", entry['passed'])
                with col3:
                    st.metric("Failed", entry.get('failed', entry['total'] - entry['passed']))
                with col4:
                    st.metric("Pass Rate", f"{pass_rate:.1f}%")

                # Show detailed results
                if st.checkbox(f"Show details", key=f"history_{i}"):
                    display_results_table(entry['results'])

        # Clear history button
        if st.button("üóëÔ∏è Clear History"):
            st.session_state.evaluation_history = []
            st.rerun()
    else:
        st.info("No evaluation history yet. Run evaluations to build history.")


with tab5:
    st.header("About SQL Query Evaluator")

    st.markdown("""
    ## Overview

    The **SQL Query Evaluator** is a comprehensive dashboard for validating SQL queries generated by chatbots, LLMs, or other automated SQL generation systems. It evaluates queries against provided business context and identifies potential issues.

    ### Key Features

    - **Context-Aware Evaluation**: Upload business rules, KPI formulas, data dictionary, and table samples
    - **Ambiguity-Aware Evaluation**: Automatically detects ambiguous questions and adjusts evaluation strictness
    - **Multi-Component Architecture**: Separate LLM calls for ambiguity detection, business rule coverage, and evaluation
    - **Intelligent Issue Detection**: Identifies 30+ types of common SQL issues
    - **Interpretation Tracking**: Shows possible interpretations and which one the SQL matches
    - **Detailed Reporting**: Get pass/fail results with confidence scores, reasoning, and evaluation paths
    - **PDF Reports**: Generate comprehensive PDF reports with LLM-powered insights
    - **Export Options**: Export results as Excel or JSON for further analysis
    - **Batch Evaluation**: Process multiple queries at once
    - **History Tracking**: Keep track of evaluation runs over time

    ---

    ## How to Use

    ### 1. Configure API (Sidebar)
    - Choose between **OpenAI** or **Azure OpenAI**
    - Enter your API key
    - Select the model (e.g., gpt-4, gpt-4o)
    - Test connection to verify setup

    ### 2. Upload Context (Sidebar)
    - **Business Rules**: Upload YAML/JSON/TXT files with business logic
    - **KPI Formulas**: Provide metric calculation formulas
    - **Data Dictionary (DDL)**: Include table schemas and column definitions
    - **Table Samples**: Upload CSV files with sample data

    ### 3. Input Questions (Tab 1)
    - Upload CSV with `question` and `sql_query` columns, or
    - Enter questions manually (up to 50 pairs), or
    - Paste JSON array

    ### 4. Run Evaluation
    - Click **Run Evaluation** button
    - Wait for LLM to evaluate each query
    - Results appear in the Results tab

    ### 5. Review Results (Tab 3)
    - View performance overview and issue distribution
    - **Ambiguity Analysis**: See breakdown by ambiguity status and business rule coverage
    - **Status Combinations**: View pass/fail rates across different evaluation paths
    - Expand individual queries to see evaluation path, interpretations, and matched interpretation
    - Filter results by status, ambiguity, or rule coverage
    - Generate PDF report for comprehensive analysis
    - Export data as Excel or JSON (includes all ambiguity and interpretation fields)

    ---

    ## Evaluation Architecture

    The evaluator uses a **multi-component architecture** that intelligently handles ambiguous questions:

    ### Component 1: Ambiguity Detection
    Analyzes the user question (without SQL or context) to detect ambiguities:
    - **Temporal ambiguity**: Unclear time references (e.g., "recent", "last month")
    - **Metric ambiguity**: Undefined metrics or KPIs
    - **Scope ambiguity**: Unclear data boundaries
    - **Aggregation ambiguity**: Unclear grouping or summarization level
    - **Entity ambiguity**: Unclear references to business entities

    ### Component 2: Business Rule Coverage Check
    For ambiguous questions, checks if business rules cover the detected ambiguities:
    - **Fully Covered**: All ambiguities have corresponding business rules
    - **Partially Covered**: Some ambiguities have business rules
    - **Not Covered**: No business rules address the ambiguities

    ### Component 3: Regular Evaluator (Strict)
    Used when question is non-ambiguous OR ambiguities are fully covered by business rules.
    - Evaluates against THE expected interpretation
    - SQL must match the specific expected behavior

    ### Component 4: Ambiguity Usecase Evaluator (Lenient)
    Used when question is ambiguous AND business rules don't fully cover the ambiguities.
    - Evaluates against ANY valid interpretation
    - SQL passes if it correctly implements any reasonable interpretation
    - Reports which interpretation was matched

    ### Evaluation Flow
    ```
    Question ‚Üí [Component 1: Ambiguity Detection]
                        ‚Üì
              Is Ambiguous? ‚îÄ‚îÄ‚îÄ No ‚îÄ‚îÄ‚Üí [Component 3: Regular Evaluator]
                        ‚Üì Yes
              [Component 2: Business Rule Coverage]
                        ‚Üì
              Fully Covered? ‚îÄ‚îÄ‚îÄ Yes ‚îÄ‚îÄ‚Üí [Component 3: Regular Evaluator]
                        ‚Üì No
              [Component 4: Ambiguity Usecase Evaluator]
    ```

    ---

    ## Evaluation Criteria

    The evaluator checks for:

    1. **Correctness**: Does the query answer the user's question?
    2. **Table Selection**: Are the right tables used?
    3. **Formula Accuracy**: Are KPIs calculated correctly?
    4. **Business Rule Compliance**: Are all business rules followed?
    5. **Column Usage**: Are correct columns selected and filtered?
    6. **Aggregation Logic**: Is aggregation appropriate?
    7. **Join Logic**: Are joins correct and necessary?
    8. **Filter Conditions**: Are WHERE clauses appropriate?

    ---

    ## Common Issue Types

    The evaluator can detect 30+ issue types across these categories:

    **Time & Date Logic** - Wrong timeframes, incorrect MTD/QTD/YTD calculations, period coverage issues

    **Aggregation & Granularity** - Mismatched aggregation levels, wrong granularity, incorrect KPI calculations

    **Filters & Conditions** - Missing required filters, incorrect filter logic, business rule violations

    **Schema & Table Usage** - Missing tables, wrong table selection, incorrect joins

    **Output Completeness** - Missing required columns, unlabeled outputs, missing date ranges

    **Function & SQL Usage** - Prohibited functions, incorrect window functions, invalid syntax

    **Business Rules** - Missing or incorrectly implemented business rules

    **Intent & Interpretation** - Ambiguous questions, incorrect interpretation

    ---

    ## Understanding Results

    ### Status Values
    - **PASS**: Query correctly answers the question
    - **FAIL**: Query has issues that need fixing
    - **ERROR**: Evaluation encountered an error

    ### Evaluation Path
    Shows which evaluator was used:
    - **non-ambiguous ‚Üí regular-evaluator**: Question was clear, strict evaluation applied
    - **ambiguous ‚Üí fully-covered ‚Üí regular-evaluator**: Ambiguous but business rules clarify, strict evaluation
    - **ambiguous ‚Üí partially-covered ‚Üí ambiguity-evaluator**: Ambiguous with partial rules, lenient evaluation
    - **ambiguous ‚Üí not-covered ‚Üí ambiguity-evaluator**: Ambiguous without rules, lenient evaluation

    ### Ambiguity Analysis (Results Tab)
    - **Ambiguity Status**: Whether the question was detected as ambiguous
    - **Ambiguities Detected**: List of specific ambiguities found (type and description)
    - **Business Rule Coverage**: Whether business rules cover the ambiguities
    - **Possible Interpretations**: Valid ways to interpret an ambiguous question
    - **Matched Interpretation**: Which interpretation the SQL actually implements (for ambiguous cases)

    ### Confidence Score
    - Ranges from 0.0 to 1.0
    - Higher = more confident in the evaluation
    - Low confidence may indicate ambiguous requirements

    ### Issues Format
    - `issue_type: Specific description`
    - Example: `timeframe_mismatch: Query uses January 2024 but user asked for current month`

    ---

    ## Tips for Best Results

    1. **Provide Rich Context**: More context = better evaluation
    2. **Clear Questions**: Well-defined questions get better evaluations
    3. **Review Failures**: Learn from common failure patterns
    4. **Use PDF Reports**: Get deep insights with LLM analysis
    5. **Track History**: Monitor improvement over time
    """)


# Footer
st.markdown("---")
st.markdown("""
<div style="text-align: center; color: #888; font-size: 0.9rem;">
    SQL Query Evaluator | Powered by OpenAI GPT | Built for validating chatbot-generated SQL queries
</div>
""", unsafe_allow_html=True)
